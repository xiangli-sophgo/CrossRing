# 硬件与互联拓扑详解

> 本文档介绍大模型训练和推理所使用的硬件加速器、互联技术和数据中心拓扑设计。

---

## 目录

1. [AI加速器概述](#一ai加速器概述)
2. [NVIDIA GPU详解](#二nvidia-gpu详解)
3. [其他AI加速器](#三其他ai加速器)
4. [互联技术详解](#四互联技术详解)
5. [服务器与机柜架构](#五服务器与机柜架构)
6. [数据中心拓扑设计](#六数据中心拓扑设计)
7. [拓扑对并行策略的影响](#七拓扑对并行策略的影响)
8. [实际部署案例](#八实际部署案例)

---

## 一、AI加速器概述

### 1.1 为什么需要专用加速器？

```
CPU vs GPU vs 专用加速器:

┌─────────────────────────────────────────────────────────────┐
│  CPU (通用处理器)                                            │
│  - 设计目标: 通用计算，低延迟                                 │
│  - 核心数: 几十个强大的核心                                   │
│  - 特点: 复杂控制逻辑，大缓存，分支预测                       │
│  - AI性能: ~1 TFLOPS (FP32)                                 │
│  - 适合: 串行任务，复杂逻辑                                   │
└─────────────────────────────────────────────────────────────┘

┌─────────────────────────────────────────────────────────────┐
│  GPU (图形处理器)                                            │
│  - 设计目标: 大规模并行计算                                   │
│  - 核心数: 数千个简单的核心                                   │
│  - 特点: SIMT架构，高显存带宽                                │
│  - AI性能: ~300-1000 TFLOPS (FP16)                         │
│  - 适合: 矩阵运算，神经网络                                   │
└─────────────────────────────────────────────────────────────┘

┌─────────────────────────────────────────────────────────────┐
│  专用AI加速器 (TPU/NPU/...)                                 │
│  - 设计目标: 专门优化AI工作负载                              │
│  - 特点: 脉动阵列，低精度计算单元                            │
│  - AI性能: 可达数百到数千TFLOPS                             │
│  - 适合: 特定AI任务                                         │
└─────────────────────────────────────────────────────────────┘
```

### 1.2 AI计算的特点

```
神经网络计算的特点:

1. 大量矩阵乘法 (GEMM)
   Y = W × X + b
   - 高度规则，易于并行
   - 占计算量90%以上

2. 高带宽需求
   - 需要快速读写大量数据
   - 显存带宽是关键瓶颈

3. 低精度容忍
   - FP16/BF16足够，INT8也可接受
   - 可用更多计算单元换精度

4. 批量处理
   - 同时处理多个样本
   - 提高计算效率
```

### 1.3 主流AI加速器对比

| 厂商 | 产品 | 算力(FP16) | 显存 | 带宽 | 互联 |
|------|------|------------|------|------|------|
| NVIDIA | H100 SXM | 990 TFLOPS | 80GB | 3.35TB/s | NVLink 900GB/s |
| NVIDIA | H200 | 990 TFLOPS | 141GB | 4.8TB/s | NVLink 900GB/s |
| NVIDIA | A100 | 312 TFLOPS | 80GB | 2.0TB/s | NVLink 600GB/s |
| AMD | MI300X | 1300 TFLOPS | 192GB | 5.3TB/s | IF 896GB/s |
| Google | TPU v5p | ~450 TFLOPS | 95GB | 4.8TB/s | ICI 4.8TB/s |
| 华为 | Ascend 910B | 320 TFLOPS | 64GB | 1.8TB/s | HCCS 392GB/s |

---

## 二、NVIDIA GPU详解

### 2.1 GPU架构演进

```
NVIDIA AI GPU发展历程:

┌──────────┬──────────┬──────────┬──────────┬──────────┐
│  Volta   │  Ampere  │  Hopper  │ Blackwell│  未来    │
│  (2017)  │  (2020)  │  (2022)  │  (2024)  │          │
├──────────┼──────────┼──────────┼──────────┼──────────┤
│  V100    │  A100    │  H100    │  B100    │   ...    │
│  125TF   │  312TF   │  990TF   │ 1800TF   │          │
│  32GB    │  80GB    │  80GB    │  192GB   │          │
└──────────┴──────────┴──────────┴──────────┴──────────┘
       ↓          ↓          ↓          ↓
    Tensor    Tensor    Tensor    Tensor
    Core v1   Core v3   Core v4   Core v5

关键创新:
- V100: 首次引入Tensor Core
- A100: 稀疏性支持, MIG多实例
- H100: Transformer Engine, FP8
- B100: 更大算力, 更多显存
```

### 2.2 H100架构详解

```
H100 SXM 规格:

┌─────────────────────────────────────────────────────────────┐
│                     H100 SXM5                               │
├─────────────────────────────────────────────────────────────┤
│                                                             │
│  计算单元:                                                   │
│  ├── SM (Streaming Multiprocessor): 132个                  │
│  ├── CUDA Cores: 16896个                                   │
│  ├── Tensor Cores: 528个 (第4代)                           │
│  └── FP8 Tensor: 1979 TFLOPS                               │
│      FP16 Tensor: 990 TFLOPS                               │
│      FP32: 67 TFLOPS                                       │
│                                                             │
│  显存系统:                                                   │
│  ├── HBM3: 80GB                                            │
│  ├── 带宽: 3.35 TB/s                                       │
│  └── ECC: 支持                                             │
│                                                             │
│  互联:                                                       │
│  ├── NVLink 4.0: 18条链路                                  │
│  ├── 总带宽: 900 GB/s (双向)                               │
│  ├── PCIe 5.0: 128 GB/s                                    │
│  └── NVSwitch支持                                          │
│                                                             │
│  功耗: 700W (SXM)                                          │
└─────────────────────────────────────────────────────────────┘
```

### 2.3 Tensor Core详解

```
Tensor Core是专门加速矩阵运算的单元:

┌─────────────────────────────────────────────────────────────┐
│                    Tensor Core操作                          │
│                                                             │
│  D = A × B + C                                              │
│                                                             │
│  A: 4×4 矩阵 (FP16)                                        │
│  B: 4×4 矩阵 (FP16)                                        │
│  C: 4×4 矩阵 (FP16/FP32)                                   │
│  D: 4×4 矩阵 (FP16/FP32)                                   │
│                                                             │
│  一个时钟周期完成整个 4×4×4 矩阵乘加运算!                   │
│  = 64次乘法 + 64次加法 = 128次运算                         │
└─────────────────────────────────────────────────────────────┘

H100 Tensor Core支持的精度:
┌──────────┬──────────┬──────────┐
│  输入A   │  输入B   │ 输出/累加 │
├──────────┼──────────┼──────────┤
│  FP8     │  FP8     │  FP16    │  ← 最高算力
│  FP16    │  FP16    │  FP32    │
│  BF16    │  BF16    │  FP32    │
│  TF32    │  TF32    │  FP32    │
│  INT8    │  INT8    │  INT32   │
└──────────┴──────────┴──────────┘
```

### 2.4 显存层次结构

```
GPU内存层次:

┌─────────────────────────────────────────────────────────────┐
│                                                             │
│  ┌─────────────┐                                           │
│  │  寄存器     │  最快，每SM 256KB                         │
│  │  Register   │  延迟: 1 cycle                            │
│  └──────┬──────┘                                           │
│         ↓                                                   │
│  ┌─────────────┐                                           │
│  │  共享内存   │  每SM 228KB (H100)                        │
│  │  Shared Mem │  延迟: ~20 cycles                         │
│  │  (SRAM)     │  带宽: ~20 TB/s                           │
│  └──────┬──────┘                                           │
│         ↓                                                   │
│  ┌─────────────┐                                           │
│  │  L2缓存     │  60MB (H100)                              │
│  │  L2 Cache   │  延迟: ~100 cycles                        │
│  └──────┬──────┘                                           │
│         ↓                                                   │
│  ┌─────────────┐                                           │
│  │  HBM3显存   │  80GB                                     │
│  │  HBM Memory │  延迟: ~300 cycles                        │
│  │             │  带宽: 3.35 TB/s                          │
│  └─────────────┘                                           │
│                                                             │
└─────────────────────────────────────────────────────────────┘

Flash Attention利用共享内存减少HBM访问!
```

### 2.5 GPU型号对比

```
NVIDIA数据中心GPU对比:

┌─────────┬────────┬────────┬────────┬─────────┬─────────┐
│ 型号    │ FP16TF │ 显存   │ 带宽   │ NVLink  │ TDP     │
├─────────┼────────┼────────┼────────┼─────────┼─────────┤
│ V100    │ 125    │ 32GB   │ 900GB/s│ 300GB/s │ 300W    │
│ A100-40 │ 312    │ 40GB   │ 1.5TB/s│ 600GB/s │ 400W    │
│ A100-80 │ 312    │ 80GB   │ 2.0TB/s│ 600GB/s │ 400W    │
│ H100SXM │ 990    │ 80GB   │ 3.35TB/s│ 900GB/s│ 700W    │
│ H100PCIe│ 756    │ 80GB   │ 2.0TB/s│ -       │ 350W    │
│ H200    │ 990    │ 141GB  │ 4.8TB/s│ 900GB/s │ 700W    │
│ B100    │ 1800   │ 192GB  │ 8TB/s  │ 1.8TB/s │ 700W    │
│ B200    │ 2250   │ 192GB  │ 8TB/s  │ 1.8TB/s │ 1000W   │
└─────────┴────────┴────────┴────────┴─────────┴─────────┘

注: SXM版本支持NVLink，PCIe版本不支持或带宽受限
```

---

## 三、其他AI加速器

### 3.1 Google TPU

```
TPU (Tensor Processing Unit):

┌─────────────────────────────────────────────────────────────┐
│                    TPU架构特点                               │
├─────────────────────────────────────────────────────────────┤
│                                                             │
│  1. 脉动阵列 (Systolic Array)                               │
│     ┌───┬───┬───┬───┐                                      │
│     │ PE│→PE│→PE│→PE│ 数据从左往右流动                     │
│     ├───┼───┼───┼───┤                                      │
│     │↓PE│→PE│→PE│→PE│ 权重从上往下流动                     │
│     ├───┼───┼───┼───┤                                      │
│     │ PE│→PE│→PE│→PE│ 结果在阵列中累加                     │
│     └───┴───┴───┴───┘                                      │
│     每个PE执行乘加操作，高效进行矩阵乘法                    │
│                                                             │
│  2. 大容量HBM内存                                           │
│     TPU v5p: 95GB HBM                                      │
│                                                             │
│  3. 高速互联 ICI (Inter-Chip Interconnect)                 │
│     TPU v5p: 4.8 TB/s 总带宽                               │
│     支持3D Torus拓扑                                       │
│                                                             │
└─────────────────────────────────────────────────────────────┘

TPU版本演进:
┌─────────┬────────┬────────┬────────┬────────┐
│ 版本    │ TPU v2 │ TPU v3 │ TPU v4 │ TPU v5p│
├─────────┼────────┼────────┼────────┼────────┤
│ BF16算力│ 45TF   │ 123TF  │ 275TF  │ 459TF  │
│ HBM     │ 8GB    │ 16GB   │ 32GB   │ 95GB   │
│ Pod规模 │ 64芯片 │ 256芯片│ 4096芯│ 8960芯│
└─────────┴────────┴────────┴────────┴────────┘
```

### 3.2 AMD Instinct

```
AMD MI300X:

┌─────────────────────────────────────────────────────────────┐
│                    MI300X规格                               │
├─────────────────────────────────────────────────────────────┤
│                                                             │
│  架构: CDNA3                                                │
│  计算单元: 304 CUs                                          │
│  FP16算力: 1307 TFLOPS                                     │
│  FP8算力: 2615 TFLOPS                                      │
│                                                             │
│  显存: 192GB HBM3                                          │
│  带宽: 5.3 TB/s                                            │
│                                                             │
│  互联: Infinity Fabric                                      │
│  带宽: 896 GB/s                                            │
│                                                             │
│  功耗: 750W                                                │
│                                                             │
│  特点:                                                      │
│  - 显存最大 (192GB vs H100 80GB)                           │
│  - 可运行更大模型或更长序列                                 │
│  - ROCm软件生态相对较弱                                     │
│                                                             │
└─────────────────────────────────────────────────────────────┘
```

### 3.3 国产AI芯片

```
主要国产AI芯片:

┌─────────────────────────────────────────────────────────────┐
│  华为昇腾 (Ascend)                                          │
├─────────────────────────────────────────────────────────────┤
│  Ascend 910B:                                               │
│  - FP16算力: ~320 TFLOPS                                   │
│  - 显存: 64GB HBM                                          │
│  - 互联: HCCS 392GB/s                                      │
│  - 生态: MindSpore框架                                      │
└─────────────────────────────────────────────────────────────┘

┌─────────────────────────────────────────────────────────────┐
│  寒武纪思元 (MLU)                                           │
├─────────────────────────────────────────────────────────────┤
│  MLU590:                                                    │
│  - INT8算力: 1024 TOPS                                     │
│  - 显存: 128GB                                             │
│  - 生态: BANG语言, MagicMind                               │
└─────────────────────────────────────────────────────────────┘

┌─────────────────────────────────────────────────────────────┐
│  海光DCU                                                    │
├─────────────────────────────────────────────────────────────┤
│  DCU Z100:                                                  │
│  - 兼容ROCm生态                                            │
│  - FP16算力: ~150 TFLOPS                                   │
│  - 显存: 32GB HBM                                          │
└─────────────────────────────────────────────────────────────┘
```

---

## 四、互联技术详解

### 4.1 互联层次结构

```
数据中心互联层次:

┌─────────────────────────────────────────────────────────────┐
│  层次1: 芯片内部                                             │
│  ├── NoC (Network on Chip)                                 │
│  └── 带宽: TB/s级别                                        │
├─────────────────────────────────────────────────────────────┤
│  层次2: GPU间 (节点内)                                       │
│  ├── NVLink / NVSwitch                                     │
│  ├── AMD Infinity Fabric                                   │
│  └── 带宽: 600-900 GB/s                                    │
├─────────────────────────────────────────────────────────────┤
│  层次3: 节点间 (机柜内)                                      │
│  ├── InfiniBand                                            │
│  ├── RoCE (RDMA over Converged Ethernet)                   │
│  └── 带宽: 100-400 Gb/s                                    │
├─────────────────────────────────────────────────────────────┤
│  层次4: 机柜间 / 数据中心                                    │
│  ├── InfiniBand                                            │
│  ├── 高速以太网                                            │
│  └── 带宽: 100-400 Gb/s                                    │
└─────────────────────────────────────────────────────────────┘

带宽差异巨大:
NVLink (900GB/s) >> InfiniBand (50GB/s) >> 以太网 (12.5GB/s)
```

### 4.2 NVLink详解

```
NVLink是NVIDIA GPU专用的高速互联:

┌─────────────────────────────────────────────────────────────┐
│                    NVLink演进                               │
├─────────────────────────────────────────────────────────────┤
│  版本      │ 每链路带宽  │ 链路数(H100)│ 总带宽           │
│  NVLink 1.0│ 20 GB/s   │ 4          │ 80 GB/s          │
│  NVLink 2.0│ 25 GB/s   │ 6          │ 150 GB/s         │
│  NVLink 3.0│ 50 GB/s   │ 12         │ 600 GB/s (A100)  │
│  NVLink 4.0│ 50 GB/s   │ 18         │ 900 GB/s (H100)  │
│  NVLink 5.0│ 100 GB/s  │ 18         │ 1.8 TB/s (B100)  │
└─────────────────────────────────────────────────────────────┘

NVLink连接方式:

1. 直连 (2-4 GPU):
   ┌─────┐     ┌─────┐
   │GPU 0│←───→│GPU 1│
   └─────┘     └─────┘

2. 全互联 (需要NVSwitch):
   ┌─────┐  ┌─────┐  ┌─────┐  ┌─────┐
   │GPU 0│  │GPU 1│  │GPU 2│  │GPU 3│
   └──┬──┘  └──┬──┘  └──┬──┘  └──┬──┘
      │        │        │        │
      └────────┴────────┴────────┘
                  │
            ┌─────────┐
            │NVSwitch │  ← 任意两GPU间全带宽
            └─────────┘
```

### 4.3 NVSwitch详解

```
NVSwitch是GPU全互联的关键:

┌─────────────────────────────────────────────────────────────┐
│                    NVSwitch架构                             │
├─────────────────────────────────────────────────────────────┤
│                                                             │
│  功能: 实现多GPU间的全互联 (All-to-All)                     │
│                                                             │
│  NVSwitch 3.0 (H100):                                      │
│  - 端口数: 64个NVLink端口                                  │
│  - 交换带宽: 3.2 TB/s                                      │
│  - 支持SHARP (网内规约)                                     │
│                                                             │
│  DGX H100 配置:                                             │
│  - 8个H100 GPU                                             │
│  - 4个NVSwitch                                             │
│  - 任意两GPU间: 900 GB/s 全带宽                            │
│                                                             │
│        GPU0  GPU1  GPU2  GPU3  GPU4  GPU5  GPU6  GPU7      │
│         │     │     │     │     │     │     │     │        │
│     ┌───┴─────┴─────┴─────┴─────┴─────┴─────┴─────┴───┐    │
│     │                  NVSwitch ×4                    │    │
│     └─────────────────────────────────────────────────┘    │
│                                                             │
│  优势:                                                      │
│  - O(1)跳数: 任意两GPU只需1跳                              │
│  - 全带宽: 不受拓扑限制                                    │
│  - 低延迟: ~1-2μs                                          │
│                                                             │
└─────────────────────────────────────────────────────────────┘
```

### 4.4 InfiniBand详解

```
InfiniBand是数据中心高性能网络:

┌─────────────────────────────────────────────────────────────┐
│                    InfiniBand规格                           │
├─────────────────────────────────────────────────────────────┤
│  版本         │ 单链路带宽  │ 4x带宽    │ 典型应用         │
│  SDR         │ 2.5 Gb/s  │ 10 Gb/s  │ 早期HPC          │
│  DDR         │ 5 Gb/s    │ 20 Gb/s  │                  │
│  QDR         │ 10 Gb/s   │ 40 Gb/s  │                  │
│  FDR         │ 14 Gb/s   │ 56 Gb/s  │                  │
│  EDR         │ 25 Gb/s   │ 100 Gb/s │ 主流             │
│  HDR         │ 50 Gb/s   │ 200 Gb/s │ AI集群           │
│  NDR         │ 100 Gb/s  │ 400 Gb/s │ 最新AI集群       │
│  XDR (预期)  │ 200 Gb/s  │ 800 Gb/s │ 下一代           │
└─────────────────────────────────────────────────────────────┘

关键技术:
1. RDMA (Remote Direct Memory Access)
   - 绕过CPU直接访问远程内存
   - 延迟: ~1-2μs
   - 减少CPU开销

2. GPUDirect RDMA
   - GPU显存直接通过网卡传输
   - 无需经过CPU和系统内存

   传统方式:
   GPU → PCIe → CPU内存 → 网卡 → 网络

   GPUDirect RDMA:
   GPU → PCIe → 网卡 → 网络  (跳过CPU内存!)
```

### 4.5 以太网方案

```
高速以太网在AI中的应用:

┌─────────────────────────────────────────────────────────────┐
│                    RoCE (RDMA over Converged Ethernet)      │
├─────────────────────────────────────────────────────────────┤
│                                                             │
│  优势:                                                      │
│  - 使用标准以太网交换机                                     │
│  - 成本低于InfiniBand                                      │
│  - 易于管理和扩展                                          │
│                                                             │
│  带宽:                                                      │
│  - 100GbE: 12.5 GB/s                                       │
│  - 200GbE: 25 GB/s                                         │
│  - 400GbE: 50 GB/s                                         │
│  - 800GbE: 100 GB/s (最新)                                 │
│                                                             │
│  挑战:                                                      │
│  - 需要无损网络配置 (PFC, ECN)                             │
│  - 对网络质量要求高                                        │
│  - 大规模时拥塞控制复杂                                    │
│                                                             │
└─────────────────────────────────────────────────────────────┘

带宽对比:
NVLink 4.0:  900 GB/s
NDR IB:      50 GB/s (400Gb/s)
400GbE:      50 GB/s

NVLink是IB/以太网的18倍!
所以TP必须放在NVLink互联的GPU间
```

### 4.6 互联带宽对比

```
各层次互联带宽对比:

┌──────────────────┬────────────┬────────────┬──────────────┐
│ 互联类型         │ 带宽       │ 延迟       │ 典型应用      │
├──────────────────┼────────────┼────────────┼──────────────┤
│ HBM3 (显存)      │ 3.35 TB/s │ ~100ns    │ 显存访问      │
│ NVLink 4.0       │ 900 GB/s  │ ~1μs      │ 节点内TP      │
│ PCIe 5.0 x16     │ 64 GB/s   │ ~1μs      │ CPU-GPU      │
│ NDR IB 400G      │ 50 GB/s   │ ~1-2μs    │ 节点间PP/DP   │
│ 400GbE RoCE      │ 50 GB/s   │ ~2-5μs    │ 节点间        │
│ 100GbE          │ 12.5 GB/s │ ~5-10μs   │ 存储/管理     │
└──────────────────┴────────────┴────────────┴──────────────┘

结论:
- TP (每层2次AllReduce): 必须用NVLink
- PP (Stage边界P2P): 可以用IB
- DP (无通信/少通信): 可以用IB/以太网
```

---

## 五、服务器与机柜架构

### 5.1 DGX系列服务器

```
NVIDIA DGX是预集成的AI服务器:

┌─────────────────────────────────────────────────────────────┐
│                    DGX H100                                 │
├─────────────────────────────────────────────────────────────┤
│                                                             │
│  GPU配置:                                                   │
│  ├── 8× H100 SXM5 (80GB)                                   │
│  ├── 总显存: 640GB                                         │
│  ├── 总算力: 7.9 PFLOPS (FP16)                             │
│  └── NVSwitch全互联                                        │
│                                                             │
│  CPU:                                                       │
│  ├── 2× Intel Xeon Platinum                                │
│  └── 112核心                                               │
│                                                             │
│  内存: 2TB DDR5                                            │
│                                                             │
│  网络:                                                      │
│  ├── 8× 400Gb/s InfiniBand (ConnectX-7)                   │
│  ├── 2× 100GbE                                            │
│  └── 总带宽: 3.2 Tb/s                                      │
│                                                             │
│  存储: 8× NVMe SSD (30TB)                                  │
│                                                             │
│  功耗: 10.2 kW                                             │
│  尺寸: 8U                                                  │
│  价格: ~$400,000                                           │
│                                                             │
└─────────────────────────────────────────────────────────────┘
```

### 5.2 DGX内部拓扑

```
DGX H100 内部连接:

                    ┌─────────────────────────────────┐
                    │      4× NVSwitch 3.0           │
                    │   (实现8 GPU全互联)             │
                    └─────────────────────────────────┘
                              ↕ NVLink 4.0
    ┌─────┐ ┌─────┐ ┌─────┐ ┌─────┐ ┌─────┐ ┌─────┐ ┌─────┐ ┌─────┐
    │GPU 0│ │GPU 1│ │GPU 2│ │GPU 3│ │GPU 4│ │GPU 5│ │GPU 6│ │GPU 7│
    └──┬──┘ └──┬──┘ └──┬──┘ └──┬──┘ └──┬──┘ └──┬──┘ └──┬──┘ └──┬──┘
       │       │       │       │       │       │       │       │
       │  PCIe │  PCIe │  PCIe │  PCIe │  PCIe │  PCIe │  PCIe │
       ↓       ↓       ↓       ↓       ↓       ↓       ↓       ↓
    ┌─────┐ ┌─────┐ ┌─────┐ ┌─────┐ ┌─────┐ ┌─────┐ ┌─────┐ ┌─────┐
    │NIC 0│ │NIC 1│ │NIC 2│ │NIC 3│ │NIC 4│ │NIC 5│ │NIC 6│ │NIC 7│
    └──┬──┘ └──┬──┘ └──┬──┘ └──┬──┘ └──┬──┘ └──┬──┘ └──┬──┘ └──┬──┘
       │       │       │       │       │       │       │       │
       └───────┴───────┴───────┴───────┴───────┴───────┴───────┘
                              ↓
                    ┌─────────────────────────────────┐
                    │     8× 400Gb/s InfiniBand       │
                    │        (到其他节点)              │
                    └─────────────────────────────────┘

每个GPU有专属网卡，实现GPU显存直接网络传输 (GPUDirect RDMA)
```

### 5.3 机柜配置

```
典型AI机柜配置:

┌─────────────────────────────────────────────────────────────┐
│                    AI训练机柜                               │
├─────────────────────────────────────────────────────────────┤
│                                                             │
│  计算节点:                                                   │
│  ├── 4× DGX H100 (32× H100 GPU)                            │
│  └── 每机柜: 32 PFLOPS (FP16)                              │
│                                                             │
│  网络设备:                                                   │
│  ├── Leaf交换机: 2× Quantum-2 (64×400G)                    │
│  └── 提供机柜内全互联                                       │
│                                                             │
│  存储:                                                       │
│  ├── 高速存储节点                                          │
│  └── 连接到存储网络                                        │
│                                                             │
│  功耗:                                                       │
│  ├── 计算: 4 × 10.2kW = 40.8kW                             │
│  ├── 网络: ~2kW                                            │
│  └── 总计: ~50kW (需要液冷)                                │
│                                                             │
│  物理尺寸:                                                   │
│  └── 42U机柜，4个DGX占32U                                  │
│                                                             │
└─────────────────────────────────────────────────────────────┘
```

---

## 六、数据中心拓扑设计

### 6.1 Fat-Tree拓扑

```
Fat-Tree是最常见的数据中心拓扑:

                        ┌─────────────────────────────┐
                        │         Core层              │
                        │  ┌───┐ ┌───┐ ┌───┐ ┌───┐  │
                        │  │ C1│ │ C2│ │ C3│ │ C4│  │
                        │  └─┬─┘ └─┬─┘ └─┬─┘ └─┬─┘  │
                        └────┼─────┼─────┼─────┼────┘
               ┌─────────────┼─────┼─────┼─────┼─────────────┐
               │             │     │     │     │             │
        ┌──────┴──────┐     │     │     │     │      ┌──────┴──────┐
        │    Pod 0    │     │     │     │     │      │    Pod N    │
        │  ┌───┬───┐  │     │     │     │     │      │  ┌───┬───┐  │
        │  │A1 │A2 │←─┼─────┘     │     └─────┼─────→│  │A1 │A2 │  │
        │  └─┬─┴─┬─┘  │           │           │      │  └─┬─┴─┬─┘  │
        │    │   │    │           │           │      │    │   │    │
        │  ┌─┴─┬─┴─┐  │           │           │      │  ┌─┴─┬─┴─┐  │
        │  │L1 │L2 │  │           │           │      │  │L1 │L2 │  │
        │  └─┬─┴─┬─┘  │           │           │      │  └─┬─┴─┬─┘  │
        │    │   │    │           │           │      │    │   │    │
        │ ┌──┴┐ ┌┴──┐ │           │           │      │ ┌──┴┐ ┌┴──┐ │
        │ │N1 │ │N2 │ │           │           │      │ │N1 │ │N2 │ │
        │ └───┘ └───┘ │           │           │      │ └───┘ └───┘ │
        └─────────────┘           │           │      └─────────────┘
                                  │
                               Spine层

层次:
- Leaf (叶子): 连接服务器
- Aggregation/Spine: 汇聚层
- Core: 核心层，连接不同Pod

特点:
- 全二分带宽 (理论上)
- 多路径冗余
- 易于扩展
```

### 6.2 Rail-Optimized拓扑

```
Rail-Optimized是为GPU集群优化的拓扑:

传统Fat-Tree问题:
- GPU通信模式是AllReduce，不是随机流量
- 传统拓扑无法利用通信模式特点

Rail-Optimized设计:
┌─────────────────────────────────────────────────────────────┐
│                                                             │
│  每个"Rail"连接所有节点的相同位置GPU                         │
│                                                             │
│  Rail 0: Node0-GPU0 ─── Node1-GPU0 ─── Node2-GPU0 ─── ...  │
│  Rail 1: Node0-GPU1 ─── Node1-GPU1 ─── Node2-GPU1 ─── ...  │
│  Rail 2: Node0-GPU2 ─── Node1-GPU2 ─── Node2-GPU2 ─── ...  │
│  ...                                                        │
│  Rail 7: Node0-GPU7 ─── Node1-GPU7 ─── Node2-GPU7 ─── ...  │
│                                                             │
│     Node 0        Node 1        Node 2        Node N       │
│  ┌─────────┐   ┌─────────┐   ┌─────────┐   ┌─────────┐    │
│  │GPU0 GPU1│   │GPU0 GPU1│   │GPU0 GPU1│   │GPU0 GPU1│    │
│  │  │   │  │   │  │   │  │   │  │   │  │   │  │   │  │    │
│  │  │   │  │   │  │   │  │   │  │   │  │   │  │   │  │    │
│  │  ├───┼──┼───┼──┼───┼──┼───┼──┼───┼──┼───┼──┼───┼──│    │
│  │  │   │  │   │  │   │  │   │  │   │  │   │  │   │  │    │
│  │GPU7 ...│   │GPU7 ...│   │GPU7 ...│   │GPU7 ...│    │
│  └─────────┘   └─────────┘   └─────────┘   └─────────┘    │
│                                                             │
└─────────────────────────────────────────────────────────────┘

优势:
- AllReduce可以在每个Rail独立进行
- 避免跨Rail通信，减少拥塞
- 更好的通信带宽利用率
```

### 6.3 3D Torus拓扑

```
3D Torus常用于超大规模集群:

┌─────────────────────────────────────────────────────────────┐
│                    3D Torus结构                             │
├─────────────────────────────────────────────────────────────┤
│                                                             │
│  每个节点连接6个邻居 (+X, -X, +Y, -Y, +Z, -Z)               │
│  边缘节点环绕连接 (形成环)                                   │
│                                                             │
│         z                                                   │
│         ↑   ┌───┐───┌───┐                                  │
│         │  ╱   ╱│  ╱   ╱│                                  │
│         │ └───┘─│─└───┘ │                                  │
│         │ │   │ │ │   │ │                                  │
│         │ │   │─│─│   │─│──→ x                             │
│         │ └───┘ │ └───┘ │                                  │
│         │╱   ╱──│╱   ╱──│                                  │
│         └───┘───└───┘   │                                  │
│        ╱               ╱                                    │
│       y               y                                     │
│                                                             │
│  TPU Pod使用3D Torus:                                       │
│  - TPU v4: 4×4×4 = 64芯片                                  │
│  - TPU v4 Pod: 12×16×16 = 3072芯片                         │
│                                                             │
│  优势:                                                      │
│  - 每个节点度数固定 (6)                                     │
│  - 扩展性好                                                 │
│  - 适合规则的通信模式                                       │
│                                                             │
│  劣势:                                                      │
│  - 对角线通信需要多跳                                       │
│  - 不如Fat-Tree通用                                        │
│                                                             │
└─────────────────────────────────────────────────────────────┘
```

### 6.4 DGX SuperPOD

```
NVIDIA DGX SuperPOD是大规模AI集群参考架构:

┌─────────────────────────────────────────────────────────────┐
│                    DGX SuperPOD配置                         │
├─────────────────────────────────────────────────────────────┤
│                                                             │
│  基础单元: 1个 Scalable Unit (SU)                          │
│  ├── 32× DGX H100                                          │
│  ├── 256× H100 GPU                                         │
│  ├── 8× Quantum-2 Leaf交换机                               │
│  └── 算力: 256 PFLOPS (FP16)                               │
│                                                             │
│  中型配置: 4× SU                                           │
│  ├── 128× DGX H100                                         │
│  ├── 1024× H100 GPU                                        │
│  └── 算力: 1 EFLOPS (FP16)                                 │
│                                                             │
│  大型配置: 32× SU                                          │
│  ├── 1024× DGX H100                                        │
│  ├── 8192× H100 GPU                                        │
│  └── 算力: 8 EFLOPS (FP16)                                 │
│                                                             │
│  网络架构:                                                   │
│  ├── 2层Fat-Tree: Leaf + Spine                             │
│  ├── 使用Quantum-2 NDR交换机                               │
│  └── Rail-Optimized设计                                    │
│                                                             │
└─────────────────────────────────────────────────────────────┘

DGX SuperPOD网络架构:

        ┌─────────────────────────────────────────────────┐
        │              Spine交换机 (Core层)                │
        │    ┌───┐ ┌───┐ ┌───┐ ┌───┐ ... ┌───┐ ┌───┐    │
        │    │S1 │ │S2 │ │S3 │ │S4 │     │S31│ │S32│    │
        │    └─┬─┘ └─┬─┘ └─┬─┘ └─┬─┘     └─┬─┘ └─┬─┘    │
        └──────┼─────┼─────┼─────┼─────────┼─────┼──────┘
               │     │     │     │         │     │
    ┌──────────┼─────┼─────┼─────┼─────────┼─────┼──────────┐
    │   Rail 0 │     │     │     │         │     │          │
    │    ┌───┐ │     │     │     │         │     │          │
    │    │L1 │←┘     │     │     │         │     │          │
    │    └─┬─┘       │     │     │         │     │          │
    │      │  ┌──────┘     │     │         │     │          │
    │   Rail 1             │     │         │     │          │
    │    ┌───┐             │     │         │     │          │
    │    │L2 │←────────────┘     │         │     │          │
    │    └─┬─┘                   │         │     │          │
    │      │                     │         │     │          │
    │    ...                    ...       ...   ...         │
    │                                                       │
    │   DGX0-GPU0  DGX1-GPU0  DGX2-GPU0 ... DGX31-GPU0     │
    │   DGX0-GPU1  DGX1-GPU1  DGX2-GPU1 ... DGX31-GPU1     │
    │     ...        ...        ...           ...          │
    │   DGX0-GPU7  DGX1-GPU7  DGX2-GPU7 ... DGX31-GPU7     │
    └───────────────────────────────────────────────────────┘
```

---

## 七、拓扑对并行策略的影响

### 7.1 并行策略与互联带宽

```
不同并行策略对带宽的需求:

┌─────────────────────────────────────────────────────────────┐
│  策略  │ 通信模式    │ 频率        │ 带宽需求  │ 放置建议   │
├────────┼─────────────┼─────────────┼───────────┼───────────┤
│  TP    │ AllReduce   │ 每层2次     │ 极高      │ NVLink内  │
│  PP    │ P2P         │ Stage边界   │ 中等      │ 可跨节点  │
│  DP    │ (推理无)    │ -           │ 低        │ 任意      │
│  EP    │ AllToAll    │ 每MoE层     │ 高        │ 优先NVLink│
│  SP    │ AllGather   │ 每Attention │ 高        │ 优先NVLink│
└─────────────────────────────────────────────────────────────┘

策略放置原则:
1. TP必须在NVLink连接的GPU间 (节点内)
2. PP可以跨节点，但要考虑气泡
3. DP放最外层，跨节点
4. EP/SP根据带宽需求决定
```

### 7.2 典型映射示例

```
示例: 32个H100，部署70B模型

方案: TP=4, PP=2, DP=4

┌─────────────────────────────────────────────────────────────┐
│                      节点0 (DGX H100)                       │
│  ┌─────────────────────────┬─────────────────────────┐      │
│  │    PP Stage 0           │    PP Stage 1           │      │
│  │  ┌────┬────┬────┬────┐  │  ┌────┬────┬────┬────┐  │      │
│  │  │GPU0│GPU1│GPU2│GPU3│  │  │GPU4│GPU5│GPU6│GPU7│  │      │
│  │  └────┴────┴────┴────┘  │  └────┴────┴────┴────┘  │      │
│  │      TP Group 0         │      TP Group 1         │      │
│  └─────────────────────────┴─────────────────────────┘      │
│                        DP Rank 0                            │
└─────────────────────────────────────────────────────────────┘
                              ↕ InfiniBand (PP通信)
┌─────────────────────────────────────────────────────────────┐
│                      节点1 (DGX H100)                       │
│                        DP Rank 1                            │
│  ...同样结构...                                              │
└─────────────────────────────────────────────────────────────┘
                              ↕
┌─────────────────────────────────────────────────────────────┐
│                      节点2 (DGX H100)                       │
│                        DP Rank 2                            │
└─────────────────────────────────────────────────────────────┘
                              ↕
┌─────────────────────────────────────────────────────────────┐
│                      节点3 (DGX H100)                       │
│                        DP Rank 3                            │
└─────────────────────────────────────────────────────────────┘

通信分析:
- TP通信 (GPU0-3, GPU4-7): 通过NVSwitch, 900GB/s
- PP通信 (GPU3→GPU4): 通过NVSwitch, 900GB/s
- DP通信 (如有): 通过IB, 50GB/s
```

### 7.3 跨节点通信优化

```
跨节点PP通信优化:

问题: PP通信需要跨节点，带宽受限

优化1: 通信隐藏
┌─────────────────────────────────────────────────────────────┐
│  计算与通信重叠:                                             │
│                                                             │
│  Stage 0: [前向计算1][前向计算2][...][发送中]               │
│  Stage 1:            [接收中][前向计算1][前向计算2][...]    │
│                                                             │
│  在计算micro-batch N时，同时发送micro-batch N-1             │
└─────────────────────────────────────────────────────────────┘

优化2: 压缩激活值
┌─────────────────────────────────────────────────────────────┐
│  减少传输数据量:                                             │
│  - FP16 → FP8/INT8                                         │
│  - 选择性传输 (只传必要的激活)                              │
└─────────────────────────────────────────────────────────────┘

优化3: 增加micro-batch
┌─────────────────────────────────────────────────────────────┐
│  更多micro-batch，更好的流水线效率:                          │
│  - micro_batch = 4, bubble = 42%                           │
│  - micro_batch = 16, bubble = 15%                          │
│  - micro_batch = 64, bubble = 4%                           │
└─────────────────────────────────────────────────────────────┘
```

### 7.4 带宽瓶颈分析

```
如何判断通信是否成为瓶颈:

计算时间 vs 通信时间:

TP通信 (每层):
通信量 = 2 × (n-1)/n × batch × seq × hidden × 2
       = 2 × 3/4 × 1 × 2048 × 8192 × 2 = 50MB

NVLink时间 = 50MB / 900GB/s = 56μs
IB时间 = 50MB / 50GB/s = 1ms  ← 慢18倍!

计算时间 (每层):
FLOPs = 24 × 1 × 2048 × 8192² = 3.3 TFLOPs
H100时间 = 3.3T / 990T × 30% = 11ms

结论:
- 用NVLink: 通信56μs << 计算11ms ✓
- 用IB: 通信1ms, 每层80次 = 80ms >> 计算11ms ✗

这就是为什么TP必须在NVLink内!
```

---

## 八、实际部署案例

### 8.1 案例1: 单节点部署LLaMA-70B

```
配置:
- 模型: LLaMA-70B
- 硬件: 1× DGX H100 (8× H100)
- 目标: 低延迟推理

┌─────────────────────────────────────────────────────────────┐
│                    部署方案                                  │
├─────────────────────────────────────────────────────────────┤
│                                                             │
│  方案: TP=8, PP=1, DP=1                                    │
│                                                             │
│  显存分析:                                                   │
│  - 模型: 140GB / 8 = 17.5GB per GPU                        │
│  - KV Cache: 按需分配                                       │
│  - 可用: 80 - 17.5 ≈ 60GB for KV Cache                     │
│                                                             │
│  性能预估:                                                   │
│  - Prefill (seq=512): ~50ms                                │
│  - Decode per token: ~6ms                                  │
│  - 吞吐量: ~1500 tokens/s (batch=8)                        │
│                                                             │
│  网络使用:                                                   │
│  - 全部在NVSwitch上，900GB/s全带宽                         │
│  - 无IB通信                                                │
│                                                             │
└─────────────────────────────────────────────────────────────┘
```

### 8.2 案例2: 多节点训练LLaMA-70B

```
配置:
- 模型: LLaMA-70B
- 硬件: 64× DGX H100 (512× H100)
- 目标: 大规模预训练

┌─────────────────────────────────────────────────────────────┐
│                    部署方案                                  │
├─────────────────────────────────────────────────────────────┤
│                                                             │
│  方案: TP=8, PP=4, DP=16                                   │
│                                                             │
│  并行度分配:                                                 │
│  - TP=8: 每个DGX节点内8卡                                  │
│  - PP=4: 4个节点组成流水线                                  │
│  - DP=16: 16个流水线副本                                   │
│  - 总GPU: 8 × 4 × 16 = 512                                 │
│                                                             │
│  通信分析:                                                   │
│  - TP: NVSwitch, 900GB/s (节点内)                          │
│  - PP: IB 400G, 50GB/s (跨节点)                            │
│  - DP: IB 400G, gradient all-reduce                        │
│                                                             │
│  训练效率:                                                   │
│  - PP气泡: micro_batch=64, bubble~6%                       │
│  - 理论MFU: ~45%                                           │
│  - 实际MFU: ~35-40%                                        │
│                                                             │
└─────────────────────────────────────────────────────────────┘

节点分布:
┌─────────────────────────────────────────────────────────────┐
│                                                             │
│  DP Group 0:                                               │
│  Node 0 (PP Stage 0) ─IB─ Node 1 (Stage 1) ─IB─ ...       │
│                                                             │
│  DP Group 1:                                               │
│  Node 4 (PP Stage 0) ─IB─ Node 5 (Stage 1) ─IB─ ...       │
│                                                             │
│  ...                                                        │
│                                                             │
│  DP Group 15:                                              │
│  Node 60 (PP Stage 0) ─IB─ Node 61 (Stage 1) ─IB─ ...     │
│                                                             │
└─────────────────────────────────────────────────────────────┘
```

### 8.3 案例3: 部署Mixtral-8x7B (MoE)

```
配置:
- 模型: Mixtral-8x7B (8专家MoE)
- 硬件: 16× H100
- 目标: 高吞吐推理

┌─────────────────────────────────────────────────────────────┐
│                    部署方案                                  │
├─────────────────────────────────────────────────────────────┤
│                                                             │
│  方案: TP=2, EP=8, PP=1, DP=1                              │
│                                                             │
│  映射:                                                       │
│  - 每2个GPU组成一个TP Group                                │
│  - 8个TP Group分别放8个Expert                              │
│  - 总GPU: 2 × 8 = 16                                       │
│                                                             │
│    Expert 0    Expert 1    ...    Expert 7                 │
│  ┌─────────┐ ┌─────────┐       ┌─────────┐                │
│  │GPU0 GPU1│ │GPU2 GPU3│  ...  │GPU14 15 │                │
│  │   TP    │ │   TP    │       │   TP    │                │
│  └─────────┘ └─────────┘       └─────────┘                │
│       ↕           ↕                 ↕                       │
│              AllToAll (EP通信)                              │
│                                                             │
│  通信分析:                                                   │
│  - TP: NVLink内, 高速                                      │
│  - EP: 跨TP Group, 需要NVSwitch或IB                        │
│                                                             │
│  显存分析:                                                   │
│  - Dense部分: ~4GB / TP = 2GB per GPU                      │
│  - Expert部分: ~7GB / GPU (每GPU 1个Expert)                │
│  - 总计: ~9GB per GPU (很轻松)                             │
│                                                             │
└─────────────────────────────────────────────────────────────┘
```

### 8.4 案例4: 超大规模GPT-4级别训练

```
配置:
- 模型: ~1.8T参数 (假设)
- 硬件: 25,000× H100 (3000+ DGX)
- 目标: 预训练

┌─────────────────────────────────────────────────────────────┐
│                    部署方案                                  │
├─────────────────────────────────────────────────────────────┤
│                                                             │
│  方案: TP=8, PP=64, DP=~50                                 │
│                                                             │
│  总GPU: 8 × 64 × ~50 ≈ 25,600                              │
│                                                             │
│  层分布 (假设120层):                                        │
│  - PP=64, 每Stage约2层                                     │
│  - 细粒度流水线，减少气泡                                   │
│                                                             │
│  挑战:                                                       │
│  1. 通信规模巨大                                           │
│     - TP: 节点内NVSwitch                                   │
│     - PP: 跨节点IB                                         │
│     - DP: 全局AllReduce                                    │
│                                                             │
│  2. 故障恢复                                               │
│     - 25000 GPU，每天必有故障                              │
│     - 需要checkpoint和快速恢复机制                         │
│                                                             │
│  3. 功耗和散热                                              │
│     - 总功耗: 25000 × 700W = 17.5 MW                       │
│     - 需要大规模液冷                                       │
│                                                             │
│  网络架构:                                                   │
│  - 多层Fat-Tree                                            │
│  - Rail-Optimized设计                                      │
│  - 专门的网络团队维护                                       │
│                                                             │
└─────────────────────────────────────────────────────────────┘
```

---

## 附录A：硬件规格速查表

### GPU规格

| 型号 | FP16 TF | 显存 | 带宽 | NVLink | TDP |
|------|---------|------|------|--------|-----|
| V100 | 125 | 32GB | 900GB/s | 300GB/s | 300W |
| A100-80 | 312 | 80GB | 2.0TB/s | 600GB/s | 400W |
| H100 SXM | 990 | 80GB | 3.35TB/s | 900GB/s | 700W |
| H200 | 990 | 141GB | 4.8TB/s | 900GB/s | 700W |
| B100 | 1800 | 192GB | 8TB/s | 1.8TB/s | 700W |
| MI300X | 1307 | 192GB | 5.3TB/s | 896GB/s | 750W |

### 网络规格

| 技术 | 带宽 | 延迟 | 适用场景 |
|------|------|------|----------|
| NVLink 4.0 | 900 GB/s | ~1μs | 节点内TP |
| PCIe 5.0 | 64 GB/s | ~1μs | CPU-GPU |
| NDR IB | 50 GB/s | ~1-2μs | 节点间PP/DP |
| 400GbE | 50 GB/s | ~2-5μs | 节点间 |
| 100GbE | 12.5 GB/s | ~5-10μs | 存储/管理 |

---

## 附录B：术语表

| 术语 | 全称 | 含义 |
|------|------|------|
| SM | Streaming Multiprocessor | GPU计算单元 |
| HBM | High Bandwidth Memory | 高带宽显存 |
| NVLink | - | NVIDIA GPU互联技术 |
| NVSwitch | - | NVLink交换芯片 |
| IB | InfiniBand | 高性能网络技术 |
| RDMA | Remote Direct Memory Access | 远程直接内存访问 |
| Fat-Tree | - | 胖树网络拓扑 |
| Rail | - | GPU对齐的网络设计 |
| MFU | Model FLOPs Utilization | 模型算力利用率 |
| Torus | - | 环形拓扑 |

---

## 附录C：常见问题

**Q: 为什么不把所有GPU用NVLink连起来？**
A: NVLink是点对点物理连接，成本高，只能连接有限数量的GPU。NVSwitch可以扩展到8-16个GPU，再大就需要网络。

**Q: InfiniBand和以太网怎么选？**
A: AI训练首选IB (低延迟、RDMA)。如果成本敏感且规模较小，RoCE以太网也可接受。

**Q: 功耗是大问题吗？**
A: 是的。1000张H100需要700kW电力，加上散热损耗，实际需要1MW+。这是数据中心的主要成本。

**Q: 国产芯片能替代吗？**
A: 目前在软件生态和单卡性能上仍有差距，但正在快速追赶。对于特定场景（如推理）已经可用。
