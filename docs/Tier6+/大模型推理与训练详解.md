# 大模型推理与训练详解

> 本文档深入介绍大模型的推理和训练过程，包括Transformer架构、计算流程、显存占用和计算量分析。

---

## 目录

1. [Transformer架构基础](#一transformer架构基础)
2. [推理过程详解](#二推理过程详解)
3. [训练过程详解](#三训练过程详解)
4. [显存占用分析](#四显存占用分析)
5. [计算量分析](#五计算量分析)
6. [性能优化技术](#六性能优化技术)
7. [实际计算示例](#七实际计算示例)

---

## 一、Transformer架构基础

### 1.1 什么是Transformer？

Transformer是现代大语言模型的核心架构，由Google在2017年提出。几乎所有主流大模型（GPT、LLaMA、Claude等）都基于Transformer。

```
Transformer的核心创新：
┌─────────────────────────────────────────────────────────┐
│  传统RNN：逐个处理token，无法并行                         │
│  输入: [A] → [B] → [C] → [D]                            │
│        ↓     ↓     ↓     ↓                              │
│       慢    慢    慢    慢                               │
├─────────────────────────────────────────────────────────┤
│  Transformer：通过Attention机制，所有token同时处理        │
│  输入: [A, B, C, D]                                     │
│            ↓                                            │
│       一次性处理（可并行）                               │
└─────────────────────────────────────────────────────────┘
```

### 1.2 Transformer整体结构

```
                    输入文本: "北京是中国的首都"
                              ↓
                    ┌─────────────────┐
                    │   Token化       │  将文字转为数字ID
                    │ [北京, 是, ...]  │  → [1234, 567, ...]
                    └────────┬────────┘
                             ↓
                    ┌─────────────────┐
                    │   Embedding     │  ID转为向量
                    │   + 位置编码    │  [1234] → [0.1, 0.2, ...]
                    └────────┬────────┘
                             ↓
            ┌────────────────────────────────┐
            │                                │
            │  ┌──────────────────────────┐  │
            │  │   Transformer Layer 1    │  │
            │  │  ┌────────────────────┐  │  │
            │  │  │  Multi-Head Attn   │  │  │
            │  │  └────────────────────┘  │  │
            │  │  ┌────────────────────┐  │  │
            │  │  │   Feed Forward     │  │  │
            │  │  └────────────────────┘  │  │
            │  └──────────────────────────┘  │
            │              ↓                  │
            │  ┌──────────────────────────┐  │
            │  │   Transformer Layer 2    │  │  × N层
            │  └──────────────────────────┘  │  (32~96层)
            │              ↓                  │
            │             ...                 │
            │              ↓                  │
            │  ┌──────────────────────────┐  │
            │  │   Transformer Layer N    │  │
            │  └──────────────────────────┘  │
            │                                │
            └────────────────┬───────────────┘
                             ↓
                    ┌─────────────────┐
                    │   LM Head       │  向量转为词表概率
                    │   (线性层)      │
                    └────────┬────────┘
                             ↓
                    输出: 下一个token的概率分布
```

### 1.3 单层Transformer详解

每一层Transformer包含两个主要模块：

```
                        输入 X
                          │
                          ↓
              ┌───────────────────────┐
              │     LayerNorm 1       │
              └───────────┬───────────┘
                          ↓
              ┌───────────────────────┐
              │  Multi-Head Attention │  ← 核心：让每个token"看到"其他token
              └───────────┬───────────┘
                          ↓
                     + 残差连接 ←───── X
                          │
                          ↓
              ┌───────────────────────┐
              │     LayerNorm 2       │
              └───────────┬───────────┘
                          ↓
              ┌───────────────────────┐
              │    Feed Forward Net   │  ← 对每个token独立做非线性变换
              │    (MLP / FFN)        │
              └───────────┬───────────┘
                          ↓
                     + 残差连接
                          │
                          ↓
                        输出
```

### 1.4 Multi-Head Attention详解

Attention机制是Transformer的核心，它让模型能够"关注"输入中的相关部分。

#### 基本Attention计算

```
Attention(Q, K, V) = softmax(Q × K^T / √d_k) × V

其中：
- Q (Query): 查询向量，"我想找什么"
- K (Key): 键向量，"我有什么"
- V (Value): 值向量，"我的内容是什么"
- d_k: Key的维度，用于缩放
```

**直观理解**：
```
输入句子: "北京是中国的首都"

当处理"首都"这个词时：
- Q = "首都"的查询向量
- K = 所有词的键向量
- V = 所有词的值向量

Attention计算：
"首都" 与 "北京" 相似度高 → 权重大
"首都" 与 "是" 相似度低 → 权重小
"首都" 与 "中国" 相似度高 → 权重大

最终：结合"北京"和"中国"的信息来理解"首都"
```

#### Multi-Head机制

不是用一个Attention，而是用**多个Attention头**，每个头关注不同的信息：

```
Multi-Head Attention:

输入 X [batch, seq, hidden]
    │
    ├──→ W_Q ──→ Q ──┐
    ├──→ W_K ──→ K ──┼──→ Split成h个头 ──→ h个Attention ──→ Concat ──→ W_O ──→ 输出
    └──→ W_V ──→ V ──┘

例如 LLaMA-70B:
- hidden_size = 8192
- num_heads = 64
- head_dim = 8192 / 64 = 128

每个头处理128维的子空间
```

**为什么要多头？**
```
头1: 关注语法关系 (主语-谓语)
头2: 关注语义关系 (同义词)
头3: 关注位置关系 (相邻词)
头4: 关注指代关系 (代词-实体)
...

多个头各司其职，综合后信息更丰富
```

#### GQA (Grouped Query Attention)

为了减少KV Cache大小，现代模型使用GQA：

```
标准MHA (Multi-Head Attention):
- Q: 64个头
- K: 64个头  ← 每个Q头对应一个K头
- V: 64个头  ← 每个Q头对应一个V头

GQA (Grouped Query Attention):
- Q: 64个头
- K: 8个头   ← 8个Q头共享一个K头
- V: 8个头   ← 8个Q头共享一个V头

好处：KV Cache减少8倍！
```

### 1.5 Feed Forward Network (FFN)

FFN是一个简单的两层MLP，对每个token独立处理：

```
FFN(x) = W2 × 激活函数(W1 × x + b1) + b2

结构：
输入 [hidden_size]
    ↓
W1 [hidden_size, intermediate_size]  ← 通常 intermediate = 4 × hidden
    ↓
激活函数 (SiLU/GELU)
    ↓
W2 [intermediate_size, hidden_size]
    ↓
输出 [hidden_size]
```

**参数量**：
```
W1: hidden × intermediate = 8192 × 32768 = 268M
W2: intermediate × hidden = 32768 × 8192 = 268M
每层FFN: ~536M参数

这是参数量最大的部分！
```

### 1.6 模型参数组成

以LLaMA-70B为例：

```
┌─────────────────────────────────────────────────────────────┐
│                    LLaMA-70B 参数分布                        │
├─────────────────────────────────────────────────────────────┤
│                                                             │
│  Embedding层:                                               │
│  ├── Token Embedding: vocab_size × hidden                   │
│  │   = 32000 × 8192 = 262M                                  │
│  └── (无位置编码参数，使用RoPE)                              │
│                                                             │
│  每层Transformer (×80层):                                    │
│  ├── Attention:                                             │
│  │   ├── W_Q: hidden × hidden = 8192 × 8192 = 67M          │
│  │   ├── W_K: hidden × kv_hidden = 8192 × 1024 = 8M (GQA)  │
│  │   ├── W_V: hidden × kv_hidden = 8192 × 1024 = 8M (GQA)  │
│  │   └── W_O: hidden × hidden = 8192 × 8192 = 67M          │
│  │   小计: 150M                                             │
│  │                                                          │
│  ├── FFN:                                                   │
│  │   ├── W_gate: hidden × inter = 8192 × 28672 = 235M      │
│  │   ├── W_up: hidden × inter = 8192 × 28672 = 235M        │
│  │   └── W_down: inter × hidden = 28672 × 8192 = 235M      │
│  │   小计: 705M                                             │
│  │                                                          │
│  └── LayerNorm: 2 × hidden = 16K (可忽略)                   │
│                                                             │
│  每层总计: ~855M                                             │
│  80层总计: 855M × 80 = 68.4B                                │
│                                                             │
│  LM Head: hidden × vocab = 8192 × 32000 = 262M              │
│  (通常与Embedding共享权重)                                   │
│                                                             │
│  总参数量: 262M + 68.4B ≈ 70B                               │
└─────────────────────────────────────────────────────────────┘
```

---

## 二、推理过程详解

### 2.1 推理的两个阶段

大模型推理分为两个截然不同的阶段：

```
用户输入: "请介绍一下北京" (假设5个token)

┌────────────────────────────────────────────────────────────┐
│                    阶段1: Prefill (预填充)                   │
│                                                            │
│  输入: [请, 介绍, 一下, 北京, <sep>]  (5个token)            │
│        ↓                                                   │
│  一次性处理所有token                                        │
│        ↓                                                   │
│  输出: 第一个token的概率分布 → "北" (采样结果)              │
│                                                            │
│  特点:                                                      │
│  - 计算密集型 (大量矩阵乘法)                                │
│  - 可高度并行 (所有token同时计算)                           │
│  - 生成KV Cache                                            │
└────────────────────────────────────────────────────────────┘
                            ↓
┌────────────────────────────────────────────────────────────┐
│                    阶段2: Decode (解码)                      │
│                                                            │
│  循环直到生成结束符:                                        │
│                                                            │
│  第1次: 输入"北" → 输出"京"                                │
│  第2次: 输入"京" → 输出"是"                                │
│  第3次: 输入"是" → 输出"中"                                │
│  ...                                                       │
│  第N次: 输入"。" → 输出"<eos>"                             │
│                                                            │
│  特点:                                                      │
│  - 每次只处理1个token                                      │
│  - 访存密集型 (需要读取所有权重和KV Cache)                  │
│  - 串行执行 (必须等上一个token生成完)                       │
│  - 复用KV Cache                                            │
└────────────────────────────────────────────────────────────┘
```

### 2.2 Prefill阶段详解

#### 计算流程

```
输入: X [batch=1, seq=5, hidden=8192]

┌─────────────────────────────────────────────────────────────┐
│ Layer 0:                                                    │
│                                                             │
│ 1. Attention:                                               │
│    Q = X × W_Q  [1, 5, 8192] × [8192, 8192] → [1, 5, 8192] │
│    K = X × W_K  [1, 5, 8192] × [8192, 1024] → [1, 5, 1024] │
│    V = X × W_V  [1, 5, 8192] × [8192, 1024] → [1, 5, 1024] │
│                                                             │
│    保存K, V到KV Cache ← 关键！                              │
│                                                             │
│    Attention计算:                                           │
│    scores = Q × K^T / √128  [1, 64, 5, 5]                  │
│    weights = softmax(scores + causal_mask)                  │
│    output = weights × V → [1, 5, 8192]                     │
│                                                             │
│ 2. FFN:                                                     │
│    gate = X × W_gate  → [1, 5, 28672]                      │
│    up = X × W_up      → [1, 5, 28672]                      │
│    hidden = SiLU(gate) × up                                │
│    output = hidden × W_down → [1, 5, 8192]                 │
│                                                             │
└─────────────────────────────────────────────────────────────┘
                            ↓
                    重复80层...
                            ↓
┌─────────────────────────────────────────────────────────────┐
│ LM Head:                                                    │
│    logits = output[:, -1, :] × W_lm  [1, 8192] × [8192, 32000] │
│    → [1, 32000]  (词表中每个词的概率)                        │
│                                                             │
│    next_token = sample(logits)  → "北"                      │
└─────────────────────────────────────────────────────────────┘
```

#### Prefill的计算量

```
每层计算量:
1. QKV投影: 3 × 2 × batch × seq × hidden × hidden
         = 3 × 2 × 1 × 5 × 8192 × 8192 = 2.0B FLOPs

2. Attention: 2 × batch × heads × seq × seq × head_dim
            = 2 × 1 × 64 × 5 × 5 × 128 = 0.4M FLOPs (很小)

3. Output投影: 2 × batch × seq × hidden × hidden
             = 2 × 1 × 5 × 8192 × 8192 = 0.67B FLOPs

4. FFN: 2 × batch × seq × hidden × (3 × intermediate)
      = 2 × 1 × 5 × 8192 × 3 × 28672 = 7.1B FLOPs

每层总计: ~10B FLOPs
80层总计: ~800B FLOPs

注: 实际Prefill时seq通常是数百到数千，计算量会大得多
```

### 2.3 Decode阶段详解

#### 计算流程

```
输入: 上一步生成的token "北" → [batch=1, seq=1, hidden=8192]
已有: KV Cache (存储了前5个token的K, V)

┌─────────────────────────────────────────────────────────────┐
│ Layer 0:                                                    │
│                                                             │
│ 1. Attention:                                               │
│    Q = X × W_Q  [1, 1, 8192] → [1, 1, 8192]  (只有1个token)│
│    K = X × W_K  [1, 1, 8192] → [1, 1, 1024]                │
│    V = X × W_V  [1, 1, 8192] → [1, 1, 1024]                │
│                                                             │
│    将新的K, V追加到KV Cache:                                │
│    K_cache: [1, 5, 1024] → [1, 6, 1024]                    │
│    V_cache: [1, 5, 1024] → [1, 6, 1024]                    │
│                                                             │
│    Attention计算 (Q与所有K计算):                            │
│    scores = Q × K_cache^T  [1, 64, 1, 6]                   │
│    weights = softmax(scores)                                │
│    output = weights × V_cache → [1, 1, 8192]               │
│                                                             │
│ 2. FFN: 同Prefill，但只处理1个token                        │
│                                                             │
└─────────────────────────────────────────────────────────────┘
```

#### Decode的特点：访存瓶颈

```
Decode每步计算量 vs 访存量:

计算量 (1个token):
- 每层: ~2B FLOPs / seq = ~400M FLOPs
- 80层: ~32B FLOPs

访存量 (必须读取):
- 模型权重: 140GB (全部权重)
- KV Cache: 随序列增长

计算强度 = FLOPs / Bytes = 32B / 140GB ≈ 0.23 FLOPs/Byte

H100规格:
- 算力: 990 TFLOPs
- 带宽: 3.35 TB/s
- 算力/带宽 = 295 FLOPs/Byte

结论: Decode是严重的访存瓶颈！
理论上GPU只能发挥 0.23/295 ≈ 0.08% 的算力
```

### 2.4 KV Cache详解

#### 为什么需要KV Cache？

```
不用KV Cache的情况 (重新计算):

生成第N个token时:
输入: [token_0, token_1, ..., token_{N-1}]
需要重新计算所有token的K和V

生成100个token的总计算量: 1+2+3+...+100 = 5050倍单token计算量
→ O(N²) 复杂度，不可接受！

使用KV Cache:
每次只计算新token的K和V，复用历史计算结果
→ O(N) 复杂度
```

#### KV Cache大小计算

```
KV Cache公式:
size = 2 × batch × seq × num_layers × num_kv_heads × head_dim × bytes

LLaMA-70B示例:
- batch = 8
- seq = 4096
- num_layers = 80
- num_kv_heads = 8 (GQA)
- head_dim = 128
- bytes = 2 (FP16)

size = 2 × 8 × 4096 × 80 × 8 × 128 × 2
     = 104 GB

这比模型本身(140GB)还要小，但仍然很大！
```

#### KV Cache的增长

```
序列长度 vs KV Cache (LLaMA-70B, batch=1):

seq=512:    ~1.6 GB
seq=2048:   ~6.5 GB
seq=4096:   ~13 GB
seq=8192:   ~26 GB
seq=32768:  ~104 GB  ← 接近单卡显存上限！
seq=131072: ~416 GB  ← 需要多卡

这就是为什么长序列场景需要特殊处理
```

### 2.5 Batch处理与调度

#### 为什么需要Batching？

```
单请求处理:
请求1 [───────────────────────]
请求2      [等待] [───────────────────────]
请求3             [等待]      [───────────────────────]

GPU大部分时间在空转（Decode阶段访存瓶颈）

Batch处理:
请求1 [───┐
请求2 [───┼─── 同时处理 ───]
请求3 [───┘

更好地利用GPU的并行计算能力
```

#### Continuous Batching

```
传统Static Batching:
┌────────────────────────────────────────────┐
│ Batch处理，等最长的请求完成才开始下一批     │
│                                            │
│ 请求1: [████████████████████]              │
│ 请求2: [████████]                          │
│ 请求3: [████████████████████████████]      │
│                                            │
│ 请求2完成后，GPU等待请求1和3               │
└────────────────────────────────────────────┘

Continuous Batching (vLLM等):
┌────────────────────────────────────────────┐
│ 请求完成后立即加入新请求                    │
│                                            │
│ 请求1: [████████████████████]              │
│ 请求2: [████████][请求4:████████████]      │
│ 请求3: [████████████████████████████]      │
│                                            │
│ 请求2完成后，立即加入请求4，不等待          │
└────────────────────────────────────────────┘

吞吐量提升显著！
```

---

## 三、训练过程详解

### 3.1 训练的三个阶段

每一步训练包含：

```
┌─────────────────────────────────────────────────────────────┐
│                    前向传播 (Forward)                        │
│                                                             │
│  输入数据 → 模型计算 → 预测结果 → 计算损失                  │
│                                                             │
│  需要保存所有中间结果（激活值）供反向传播使用                │
└─────────────────────────────────────────────────────────────┘
                            ↓
┌─────────────────────────────────────────────────────────────┐
│                    反向传播 (Backward)                       │
│                                                             │
│  从损失出发，逐层计算梯度                                    │
│  loss → ∂loss/∂W_N → ∂loss/∂W_{N-1} → ... → ∂loss/∂W_1    │
│                                                             │
│  需要读取激活值，计算并存储梯度                              │
└─────────────────────────────────────────────────────────────┘
                            ↓
┌─────────────────────────────────────────────────────────────┐
│                    优化器更新 (Optimizer)                    │
│                                                             │
│  根据梯度更新参数：                                          │
│  W = W - lr × optimizer(gradient)                          │
│                                                             │
│  AdamW需要额外存储: m (一阶动量), v (二阶动量)              │
└─────────────────────────────────────────────────────────────┘
```

### 3.2 前向传播详解

```
前向传播与推理的Prefill类似，但有关键区别：

推理 (Prefill):
- 只需要最后一层输出
- 不需要保存中间结果
- 可以边计算边丢弃

训练 (Forward):
- 需要保存每一层的激活值
- 这些激活值在反向传播时使用
- 显存占用大幅增加

┌─────────────────────────────────────────────────────────────┐
│                    需要保存的激活值                          │
│                                                             │
│  每层需要保存:                                               │
│  1. 输入激活 X                                              │
│  2. Attention的Q, K, V                                      │
│  3. Attention的softmax输出                                  │
│  4. FFN的中间激活                                           │
│  5. 残差连接的输入                                          │
│                                                             │
│  激活值大小 ≈ batch × seq × hidden × 层数 × 若干系数       │
└─────────────────────────────────────────────────────────────┘
```

### 3.3 反向传播详解

```
反向传播计算梯度的过程:

损失函数 L = CrossEntropy(预测, 标签)

从最后一层开始，逐层计算:
∂L/∂W = ∂L/∂output × ∂output/∂W

┌─────────────────────────────────────────────────────────────┐
│ Layer N (最后一层):                                         │
│   读取: 保存的激活值                                        │
│   计算: 该层权重的梯度 ∂L/∂W_N                             │
│   传递: 输入的梯度 ∂L/∂X_N 给上一层                        │
├─────────────────────────────────────────────────────────────┤
│ Layer N-1:                                                  │
│   接收: ∂L/∂X_N                                            │
│   读取: 保存的激活值                                        │
│   计算: ∂L/∂W_{N-1}                                        │
│   传递: ∂L/∂X_{N-1} 给上一层                               │
├─────────────────────────────────────────────────────────────┤
│ ...重复直到第一层...                                        │
└─────────────────────────────────────────────────────────────┘

反向传播的计算量 ≈ 2 × 前向传播
(需要计算两个方向的梯度)
```

### 3.4 优化器详解

#### SGD (最简单)

```
W_new = W_old - lr × gradient

只需存储: 梯度 (与参数同大小)
```

#### Adam/AdamW (最常用)

```
AdamW更新公式:
m = β1 × m + (1 - β1) × gradient        # 一阶动量
v = β2 × v + (1 - β2) × gradient²       # 二阶动量
m_hat = m / (1 - β1^t)                  # 偏差修正
v_hat = v / (1 - β2^t)
W_new = W_old - lr × m_hat / (√v_hat + ε) - wd × W_old

需要存储:
- m: 一阶动量，与参数同大小
- v: 二阶动量，与参数同大小

即: 优化器状态 = 2 × 参数量
```

### 3.5 训练 vs 推理对比

| 方面 | 推理 | 训练 |
|------|------|------|
| 计算方向 | 只有前向 | 前向 + 反向 |
| 激活值 | 可以丢弃 | 必须保存 |
| 梯度 | 不需要 | 需要存储 |
| 优化器 | 不需要 | 需要存储状态 |
| 计算量 | 1× | 3× (前向1 + 反向2) |
| 显存 | 较小 | 大很多 |

---

## 四、显存占用分析

### 4.1 推理时的显存占用

```
┌─────────────────────────────────────────────────────────────┐
│                    推理显存组成                              │
├─────────────────────────────────────────────────────────────┤
│                                                             │
│  1. 模型权重 (Model Weights)                                │
│     = 参数量 × 每参数字节数                                 │
│     = 70B × 2 (FP16) = 140 GB                              │
│                                                             │
│  2. KV Cache                                                │
│     = 2 × batch × seq × layers × kv_heads × head_dim × 2  │
│     例: 2 × 8 × 4096 × 80 × 8 × 128 × 2 = 104 GB          │
│                                                             │
│  3. 激活值 (峰值)                                           │
│     ≈ batch × seq × hidden × 若干层                        │
│     例: 8 × 4096 × 8192 × 4 × 2 ≈ 2 GB                    │
│                                                             │
│  4. 临时缓冲区                                              │
│     ≈ 1-5 GB                                               │
│                                                             │
├─────────────────────────────────────────────────────────────┤
│  总计 (LLaMA-70B, batch=8, seq=4096):                      │
│  140 + 104 + 2 + 2 ≈ 248 GB                                │
│                                                             │
│  单卡H100 (80GB) 无法容纳，需要至少4卡                      │
└─────────────────────────────────────────────────────────────┘
```

### 4.2 训练时的显存占用

```
┌─────────────────────────────────────────────────────────────┐
│                    训练显存组成                              │
├─────────────────────────────────────────────────────────────┤
│                                                             │
│  1. 模型权重                                                │
│     FP32: 70B × 4 = 280 GB                                 │
│     BF16混合精度: 70B × 2 = 140 GB (+ FP32主权重280GB)     │
│                                                             │
│  2. 梯度                                                    │
│     = 参数量 × 每参数字节数                                 │
│     FP32: 70B × 4 = 280 GB                                 │
│                                                             │
│  3. 优化器状态 (AdamW)                                      │
│     = 2 × 参数量 × 4字节 (必须FP32)                        │
│     = 2 × 70B × 4 = 560 GB                                 │
│                                                             │
│  4. 激活值                                                  │
│     = batch × seq × hidden × layers × 系数                 │
│     LLaMA-70B, batch=1, seq=2048:                          │
│     ≈ 1 × 2048 × 8192 × 80 × 34 × 2 ≈ 90 GB              │
│     (系数34来自需要保存的各种中间值)                        │
│                                                             │
│  5. 临时缓冲区                                              │
│     ≈ 10-20 GB                                             │
│                                                             │
├─────────────────────────────────────────────────────────────┤
│  总计 (LLaMA-70B, BF16混合精度, batch=1):                  │
│  140 + 280 + 280 + 560 + 90 + 15 ≈ 1365 GB                │
│                                                             │
│  需要至少 1365/80 ≈ 18张 H100！                            │
│  考虑到并行开销，实际需要32-64张                            │
└─────────────────────────────────────────────────────────────┘
```

### 4.3 显存优化技术

#### 激活值重计算 (Activation Checkpointing)

```
原理: 不保存所有激活值，需要时重新计算

原始方式:
Forward: 计算并保存所有激活值 a1, a2, ..., an
Backward: 使用保存的激活值计算梯度

激活重计算:
Forward: 只保存部分检查点 a1, a_k, a_2k, ...
Backward: 需要某层激活时，从最近检查点重新计算

trade-off:
- 显存减少: 最多可减少到 √N
- 计算增加: 约增加33%
```

#### 梯度累积 (Gradient Accumulation)

```
原理: 用小batch多次累积梯度，等效于大batch

for i in range(accumulation_steps):
    loss = forward(micro_batch_i)
    loss.backward()  # 梯度累积，不清零

optimizer.step()     # 累积完后才更新
optimizer.zero_grad()

好处:
- 显存只需要存储micro_batch的激活值
- 等效batch_size = micro_batch × accumulation_steps
```

#### ZeRO优化 (DeepSpeed)

```
ZeRO-1: 分片优化器状态
- 优化器状态分到各GPU
- 显存节省: (优化器状态) / DP

ZeRO-2: 分片优化器状态 + 梯度
- 梯度也分片存储
- 显存节省: (优化器 + 梯度) / DP

ZeRO-3: 分片所有状态
- 模型参数也分片
- 显存节省: (参数 + 优化器 + 梯度) / DP
- 需要通信来gather参数
```

---

## 五、计算量分析

### 5.1 FLOPs基础概念

```
FLOP = Floating Point Operation (浮点运算)
FLOPs = 总运算次数
FLOPS = FLOPs per Second (每秒运算次数)

矩阵乘法 A[m,k] × B[k,n] 的FLOPs:
= 2 × m × k × n
(每个输出元素需要k次乘法和k-1次加法 ≈ 2k次运算)
```

### 5.2 Transformer每层计算量

```
输入: [batch, seq, hidden]
假设: batch=B, seq=S, hidden=H, intermediate=I, heads=h

┌─────────────────────────────────────────────────────────────┐
│                    Attention计算量                          │
├─────────────────────────────────────────────────────────────┤
│  QKV投影:                                                   │
│  Q = X × W_Q: 2 × B × S × H × H                            │
│  K = X × W_K: 2 × B × S × H × H_kv (GQA时H_kv < H)        │
│  V = X × W_V: 2 × B × S × H × H_kv                         │
│                                                             │
│  Attention计算:                                             │
│  scores = Q × K^T: 2 × B × h × S × S × (H/h)              │
│  output = scores × V: 2 × B × h × S × S × (H/h)           │
│                                                             │
│  输出投影:                                                  │
│  O = Attn × W_O: 2 × B × S × H × H                         │
│                                                             │
│  Attention总计 ≈ 4 × B × S × H² + 4 × B × S² × H          │
│  (忽略GQA简化)                                              │
└─────────────────────────────────────────────────────────────┘

┌─────────────────────────────────────────────────────────────┐
│                    FFN计算量                                │
├─────────────────────────────────────────────────────────────┤
│  gate = X × W_gate: 2 × B × S × H × I                      │
│  up = X × W_up: 2 × B × S × H × I                          │
│  down = (gate×up) × W_down: 2 × B × S × I × H              │
│                                                             │
│  FFN总计 = 6 × B × S × H × I                               │
│  (通常 I ≈ 4H，所以 ≈ 24 × B × S × H²)                     │
└─────────────────────────────────────────────────────────────┘

每层总计 ≈ 4BSH² + 4BS²H + 24BSH² = 28BSH² + 4BS²H
        ≈ 24BSH² (当S << H时，S²项可忽略)
```

### 5.3 整体计算量

#### 推理计算量

```
Prefill (处理P个输入token):
FLOPs_prefill = L × (24 × B × P × H² + 4 × B × P² × H)
             ≈ 24 × L × B × P × H² (当P较小时)
             ≈ 2 × 参数量 × P (近似)

Decode (生成D个token，每次1个):
FLOPs_decode_per_token = L × 24 × B × 1 × H²
                       ≈ 2 × 参数量

FLOPs_decode_total = D × 2 × 参数量
                   ≈ 2 × D × 参数量

总计: FLOPs ≈ 2 × 参数量 × (P + D)
```

#### 训练计算量

```
前向传播: ≈ 2 × 参数量 × tokens
反向传播: ≈ 4 × 参数量 × tokens (约2倍前向)

每步训练: ≈ 6 × 参数量 × tokens

训练1个epoch (假设1T tokens):
FLOPs = 6 × 70B × 1T = 420 ZFLOPs

用1000张H100 (990 TFLOPs × 30%利用率):
时间 = 420 ZFLOPs / (1000 × 990T × 0.3) / 3600 / 24
    ≈ 16天
```

### 5.4 计算强度分析

```
计算强度 = FLOPs / 访存字节数

Prefill阶段:
FLOPs = 2 × params × seq
访存 = params × 2 (读取权重)
计算强度 = seq (与序列长度成正比)

seq=1: 计算强度=1 (访存瓶颈)
seq=1000: 计算强度=1000 (计算瓶颈)

Decode阶段:
FLOPs = 2 × params
访存 = params × 2 + KV_cache
计算强度 ≈ 1 (严重访存瓶颈)

结论:
- Prefill: 长序列时计算瓶颈，可充分利用GPU
- Decode: 总是访存瓶颈，GPU利用率低
```

---

## 六、性能优化技术

### 6.1 Flash Attention

```
传统Attention问题:
1. 存储完整的 S×S attention矩阵
2. 显存占用 O(S²)
3. 访存次数多

Flash Attention原理:
1. 分块计算，每次只处理一小块
2. 利用GPU SRAM (快但小) 而非HBM (慢但大)
3. 融合多个操作减少访存

┌─────────────────────────────────────────────────────────────┐
│  传统方式:                                                   │
│  Q, K, V → [HBM] → 计算S → [HBM] → softmax → [HBM] → ×V    │
│                   多次读写HBM                               │
├─────────────────────────────────────────────────────────────┤
│  Flash Attention:                                           │
│  分块加载Q, K, V到SRAM → 计算 → 直接输出                    │
│                   最小化HBM访问                              │
└─────────────────────────────────────────────────────────────┘

效果:
- 显存: O(S²) → O(S)
- 速度: 2-4倍提升
```

### 6.2 量化 (Quantization)

```
原理: 用更少的bit表示参数

FP16 (16bit) → INT8 (8bit): 显存减半，速度提升
FP16 (16bit) → INT4 (4bit): 显存1/4，速度提升更多

┌─────────────────────────────────────────────────────────────┐
│  量化方式:                                                   │
│                                                             │
│  动态量化: 推理时量化，简单但有开销                          │
│  静态量化: 预先量化，需要校准数据                            │
│  QAT: 训练时感知量化，精度最好                              │
│                                                             │
│  常用方案:                                                   │
│  - W8A8: 权重INT8，激活INT8                                │
│  - W4A16: 权重INT4，激活FP16 (GPTQ, AWQ)                   │
└─────────────────────────────────────────────────────────────┘

精度影响:
- INT8: 几乎无损
- INT4: 有一定损失，但可接受
- INT2: 损失较大，需要特殊处理
```

### 6.3 Speculative Decoding

```
原理: 用小模型"猜测"多个token，大模型验证

传统Decode:
大模型生成token 1 → 大模型生成token 2 → ... (串行)

Speculative Decoding:
小模型猜测 [t1, t2, t3, t4] → 大模型一次验证 → 接受前3个，拒绝t4
                                            ↓
                                    一次生成3个token！

┌─────────────────────────────────────────────────────────────┐
│  验证过程:                                                   │
│                                                             │
│  小模型猜测: [A, B, C, D]                                   │
│  大模型计算: P(A|ctx), P(B|ctx,A), P(C|ctx,A,B), ...       │
│                                                             │
│  如果P_大(t) ≈ P_小(t)，接受该token                         │
│  如果P_大(t) >> P_小(t)，从大模型重新采样                    │
│                                                             │
│  平均接受率: 70-80% (取决于小模型质量)                       │
└─────────────────────────────────────────────────────────────┘

加速比: 2-3倍 (Decode阶段)
```

### 6.4 PagedAttention (vLLM)

```
原理: 像操作系统管理内存一样管理KV Cache

传统方式:
- 每个请求预分配最大长度的KV Cache
- 大量显存浪费 (预分配但未使用)

PagedAttention:
- KV Cache分成固定大小的"页"
- 按需分配，动态增长
- 不同请求可以共享页 (如相同的system prompt)

┌─────────────────────────────────────────────────────────────┐
│  显存管理:                                                   │
│                                                             │
│  请求1: [页1][页2][页3]                                     │
│  请求2: [页4][页5]                                          │
│  请求3: [页1][页6][页7][页8]  ← 页1与请求1共享!             │
│                                                             │
│  显存利用率: 从50-60% → 90%+                                │
└─────────────────────────────────────────────────────────────┘
```

---

## 七、实际计算示例

### 示例1：LLaMA-70B推理分析

```
配置:
- 模型: LLaMA-70B
- 硬件: 8×H100 (TP=8)
- 请求: batch=8, input=512, output=256

┌─────────────────────────────────────────────────────────────┐
│                    Prefill阶段                              │
├─────────────────────────────────────────────────────────────┤
│  计算量:                                                    │
│  FLOPs = 2 × 70B × 512 × 8 = 573 TFLOPs                   │
│                                                             │
│  计算时间 (8×H100, 990TFLOPs each, 50%利用率):             │
│  = 573T / (8 × 990T × 0.5) = 144 ms                        │
│                                                             │
│  TP通信量 (每层2次AllReduce):                               │
│  = 2 × 80 × 2 × 7/8 × 8 × 512 × 8192 × 2 = 1.8 TB        │
│                                                             │
│  通信时间 (NVLink 900GB/s双向):                             │
│  = 1.8T / 900G = 2 ms (与计算重叠)                         │
│                                                             │
│  Prefill延迟 ≈ 150 ms                                      │
└─────────────────────────────────────────────────────────────┘

┌─────────────────────────────────────────────────────────────┐
│                    Decode阶段                               │
├─────────────────────────────────────────────────────────────┤
│  每token计算量:                                             │
│  FLOPs = 2 × 70B × 1 × 8 = 1.12 TFLOPs                    │
│                                                             │
│  访存量:                                                    │
│  权重: 140GB / 8 = 17.5 GB per GPU                         │
│  KV Cache: 随序列增长                                       │
│                                                             │
│  访存时间 (HBM 3.35TB/s):                                  │
│  = 17.5G / 3.35T = 5.2 ms                                  │
│                                                             │
│  每token延迟 ≈ 5-6 ms (访存瓶颈)                           │
│                                                             │
│  256 tokens总延迟 = 256 × 5.5 = 1.4 s                      │
└─────────────────────────────────────────────────────────────┘

端到端延迟 = 150ms + 1.4s ≈ 1.55s
吞吐量 = 8 × 256 / 1.55 ≈ 1320 tokens/s
```

### 示例2：训练显存估算

```
配置:
- 模型: LLaMA-7B
- 训练: BF16混合精度, batch=32, seq=2048

┌─────────────────────────────────────────────────────────────┐
│                    显存占用估算                              │
├─────────────────────────────────────────────────────────────┤
│                                                             │
│  1. 模型权重 (BF16):                                        │
│     7B × 2 = 14 GB                                         │
│                                                             │
│  2. 主权重副本 (FP32, 优化器用):                            │
│     7B × 4 = 28 GB                                         │
│                                                             │
│  3. 梯度 (FP32):                                            │
│     7B × 4 = 28 GB                                         │
│                                                             │
│  4. 优化器状态 (AdamW, FP32):                               │
│     2 × 7B × 4 = 56 GB                                     │
│                                                             │
│  5. 激活值 (使用激活重计算):                                 │
│     ≈ batch × seq × hidden × layers × 2 × 2               │
│     = 32 × 2048 × 4096 × 32 × 2 × 2 / 重计算因子          │
│     ≈ 34 GB (重计算后)                                     │
│                                                             │
│  总计: 14 + 28 + 28 + 56 + 34 = 160 GB                     │
│                                                             │
│  单卡80GB不够，需要至少2卡 (使用ZeRO-2)                     │
└─────────────────────────────────────────────────────────────┘
```

### 示例3：通信量分析

```
配置:
- 模型: LLaMA-70B
- 部署: TP=4, PP=2
- 推理: batch=1, seq=2048

┌─────────────────────────────────────────────────────────────┐
│                    TP通信量 (每stage)                        │
├─────────────────────────────────────────────────────────────┤
│  每层AllReduce:                                             │
│  = 2 × (4-1)/4 × 1 × 2048 × 8192 × 2                      │
│  = 50 MB                                                    │
│                                                             │
│  每stage 40层:                                              │
│  = 50 × 40 × 2 (Attn+FFN) = 4 GB                          │
│                                                             │
│  通信时间 (NVLink 450GB/s单向):                             │
│  = 4G / 450G × 1000 = 9 ms                                 │
└─────────────────────────────────────────────────────────────┘

┌─────────────────────────────────────────────────────────────┐
│                    PP通信量                                  │
├─────────────────────────────────────────────────────────────┤
│  Stage边界传递激活值:                                        │
│  = 1 × 2048 × 8192 × 2 = 32 MB                             │
│                                                             │
│  通信时间 (假设IB 100GB/s):                                 │
│  = 32M / 100G × 1000 = 0.3 ms                              │
└─────────────────────────────────────────────────────────────┘

结论: TP通信远大于PP通信，应优先将TP放在高带宽互联上
```

---

## 附录A：常用公式速查

### 显存公式

```
模型显存 = 参数量 × bytes_per_param

KV Cache = 2 × batch × seq × layers × kv_heads × head_dim × 2

训练激活值 ≈ batch × seq × hidden × layers × 34 × 2

优化器状态 (AdamW) = 2 × 参数量 × 4
```

### 计算量公式

```
每层FLOPs ≈ 24 × batch × seq × hidden²

总FLOPs ≈ 2 × 参数量 × tokens

训练FLOPs ≈ 6 × 参数量 × tokens
```

### 通信量公式

```
AllReduce = 2 × (n-1)/n × data_size

TP通信/层 = 2 × AllReduce × batch × seq × hidden × 2

PP通信/边界 = batch × seq × hidden × 2
```

### 延迟公式

```
计算延迟 = FLOPs / (峰值算力 × 利用率)

通信延迟 = 通信量 / 带宽 + 启动延迟

Decode延迟 ≈ 模型大小 / 显存带宽 (访存瓶颈)
```

---

## 附录B：性能指标参考

### 延迟基准

| 场景 | 首token延迟 | 每token延迟 |
|------|-------------|-------------|
| 7B本地 | 50-100ms | 20-50ms |
| 70B 8卡 | 100-200ms | 5-10ms |
| API服务 | 200-500ms | 10-30ms |

### 吞吐量基准

| 模型 | 硬件 | 吞吐量 |
|------|------|--------|
| 7B | 单A100 | 1000-2000 tok/s |
| 70B | 8×H100 | 500-1500 tok/s |
| 70B | 8×A100 | 300-800 tok/s |

### GPU利用率参考

| 阶段 | 典型利用率 | 瓶颈 |
|------|------------|------|
| Prefill | 40-60% | 计算 |
| Decode | 5-15% | 访存 |
| 训练 | 30-50% | 混合 |

---

## 附录C：术语表

| 术语 | 含义 |
|------|------|
| Prefill | 处理输入prompt的阶段 |
| Decode | 逐token生成输出的阶段 |
| KV Cache | 存储历史K、V避免重复计算 |
| FLOPs | 浮点运算次数 |
| 激活值 | 前向传播的中间结果 |
| 梯度 | 参数对损失的导数 |
| 混合精度 | 用低精度计算，高精度存储 |
| Flash Attention | 优化的Attention实现 |
| 量化 | 用更少bit表示参数 |
