{
  "nodes": [
    {
      "id": "scale_up",
      "name": "Scale Up",
      "definition": "通过向单个系统添加更多资源（如CPU、内存、GPU）来提升性能的垂直扩展方式。适用于单机内资源升级，但受限于单节点的物理上限。",
      "category": "parallel",
      "source": "Hill, M.D. (1990). What is Scalability? ACM SIGARCH Computer Architecture News",
      "notes": "又称垂直扩展(Vertical Scaling)，典型场景如NVLink多GPU互联"
    },
    {
      "id": "scale_out",
      "name": "Scale Out",
      "definition": "通过添加更多独立节点并分布工作负载来提升系统容量的水平扩展方式。适用于分布式系统，理论上可无限扩展，但需要解决通信和一致性问题。",
      "category": "parallel",
      "source": "Hill, M.D. (1990). What is Scalability? ACM SIGARCH Computer Architecture News",
      "notes": "又称水平扩展(Horizontal Scaling)，典型场景如跨服务器InfiniBand互联"
    },
    {
      "id": "tp",
      "name": "TP",
      "definition": "Tensor Parallelism（张量并行）是一种模型并行策略，通过将单层权重矩阵沿行或列维度切分到多个GPU，使每个GPU只存储和计算矩阵的一部分。主要用于Transformer中的Attention和FFN层切分。",
      "category": "parallel",
      "source": "Shoeybi et al. (2019). Megatron-LM: Training Multi-Billion Parameter Language Models Using Model Parallelism. arXiv:1909.08053",
      "notes": "每次前向/反向需要AllReduce同步，通信开销与序列长度×隐藏维度成正比"
    },
    {
      "id": "sp",
      "name": "SP",
      "definition": "Sequence Parallelism（序列并行）是一种激活内存优化策略，通过将Transformer层中序列维度的激活值切分到多个GPU，减少每个GPU的内存占用。通常与TP配合使用，切分LayerNorm和Dropout等非TP操作的激活。",
      "category": "parallel",
      "source": "Korthikanti et al. (2023). Reducing Activation Recomputation in Large Transformer Models. MLSys 2023",
      "notes": "SP是TP的补充，主要优化TP无法切分的算子的激活内存"
    },
    {
      "id": "ep",
      "name": "EP",
      "definition": "Expert Parallelism（专家并行）是MoE模型专用的并行策略，将不同专家子网络分布到不同GPU上。每个token通过门控网络路由到特定专家，需要AllToAll通信进行token重分布。",
      "category": "parallel",
      "source": "Fedus et al. (2022). Switch Transformers: Scaling to Trillion Parameter Models with Simple and Efficient Sparsity. JMLR",
      "notes": "EP的通信模式为AllToAll，与TP的AllReduce不同，对网络拓扑要求不同"
    },
    {
      "id": "dp",
      "name": "DP",
      "definition": "Data Parallelism（数据并行）是分布式训练中最基础的并行策略，在多个GPU上复制完整模型，将训练数据的batch切分给各GPU并行处理。每个step结束后需要AllReduce同步梯度。",
      "category": "parallel",
      "source": "Dean et al. (2012). Large Scale Distributed Deep Networks. NIPS 2012",
      "notes": "最简单的并行方式，但每个GPU需存储完整模型，内存效率低"
    },
    {
      "id": "pp",
      "name": "PP",
      "definition": "Pipeline Parallelism（流水线并行）是一种模型并行策略，将网络按层分组为多个Stage分配到不同GPU，训练数据以micro-batch形式流水执行。通过流水线并行可减少气泡(bubble)时间。",
      "category": "parallel",
      "source": "Huang et al. (2019). GPipe: Efficient Training of Giant Neural Networks using Pipeline Parallelism. NeurIPS 2019",
      "notes": "Interleaved 1F1B调度可将bubble率降至O(1/m)，m为micro-batch数"
    },
    {
      "id": "内存语义",
      "name": "内存语义",
      "definition": "Memory Semantics是一种通信语义，允许发起方通过Load/Store/Atomic操作直接访问远程节点的内存空间，无需远程CPU参与。是RDMA的核心特性，提供单边操作能力。",
      "category": "communication",
      "source": "InfiniBand Architecture Specification Volume 1, Release 1.4",
      "notes": "RDMA Write/Read即为内存语义操作，绕过远程CPU"
    },
    {
      "id": "消息语义",
      "name": "消息语义",
      "definition": "Message Semantics（消息语义）是一种通信语义，发送方发送消息，接收方必须预先Post Receive缓冲区来接收。双方CPU都需参与，提供可靠的消息传递保证。",
      "category": "communication",
      "source": "InfiniBand Architecture Specification Volume 1, Release 1.4",
      "notes": "MPI Send/Recv即为消息语义，需要双边操作配合"
    },
    {
      "id": "ras",
      "name": "RAS",
      "definition": "RAS（Reliability, Availability, Serviceability）是衡量系统健壮性的三个属性：可靠性（无故障运行能力）、可用性（持续服务能力）、可维护性（故障诊断和修复能力）。是企业级系统的核心设计目标。",
      "category": "system",
      "source": "IBM System/360 RAS Architecture (1964); IEEE Std 1413-2010",
      "notes": "ECC内存、热插拔、故障预测等都是RAS技术"
    },
    {
      "id": "pod",
      "name": "Pod",
      "definition": "Pod是数据中心中由多台机架（Rack）组成的模块化部署单元，通常包含统一的网络、电力和制冷设计。典型AI Pod包含数十到数百个GPU，形成独立的计算域。",
      "category": "hardware",
      "source": "Google Data Center Design; NVIDIA DGX SuperPOD Reference Architecture",
      "notes": "NVIDIA SuperPOD由多个DGX节点组成，是大模型训练的基本单元"
    },
    {
      "id": "rack",
      "name": "Rack",
      "definition": "Rack（机架）是用于安装服务器、网络和存储设备的标准化结构，遵循EIA-310标准（19英寸宽度）。提供统一的物理尺寸、供电和布线承载能力，是数据中心最基本的物理部署单元。",
      "category": "hardware",
      "source": "EIA-310-E Standard; ANSI/TIA-942 Data Center Standards",
      "notes": "标准机架高度42U，可容纳约40台1U服务器"
    },
    {
      "id": "server",
      "name": "Server",
      "definition": "Server（服务器）是提供计算、存储或网络服务的专用计算机系统，具备高可靠性、远程管理和持续运行能力。AI服务器通常配备多GPU和高带宽网卡。",
      "category": "hardware",
      "source": "IEEE Computer Society; Data Center Infrastructure Standards",
      "notes": "典型AI服务器如NVIDIA DGX配备8×H100 GPU和NVSwitch"
    },
    {
      "id": "1u_2u_4u",
      "name": "1U/2U/4U",
      "definition": "U（Rack Unit）是机架高度单位，1U=1.75英寸（44.45mm）。1U服务器适合高密度计算，4U及以上适合配置多GPU和扩展卡的AI服务器。",
      "category": "hardware",
      "source": "EIA-310-E Standard",
      "notes": "AI服务器通常为4U-8U以容纳多GPU和散热系统"
    },
    {
      "id": "事务",
      "name": "事务",
      "definition": "Transaction（事务）是系统中一组被视为不可分割的操作集合，具有ACID特性：原子性、一致性、隔离性、持久性。在总线协议中，事务指一次完整的请求-响应交互。",
      "category": "system",
      "source": "Gray & Reuter (1993). Transaction Processing: Concepts and Techniques. Morgan Kaufmann",
      "notes": "PCIe/AXI中的Transaction包含地址、命令和数据的完整交换"
    },
    {
      "id": "ecc",
      "name": "ECC",
      "definition": "ECC（Error-Correcting Code）是一类用于检测和纠正数据错误的编码技术。ECC内存通过额外的校验位实现单比特纠错、双比特检错（SECDED），是服务器内存的标准配置。",
      "category": "protocol",
      "source": "Hamming, R.W. (1950). Error Detecting and Error Correcting Codes. Bell System Technical Journal",
      "notes": "HBM3使用on-die ECC，GPU内存也支持ECC模式"
    },
    {
      "id": "crc",
      "name": "CRC",
      "definition": "CRC（Cyclic Redundancy Check）是一种基于多项式除法的差错检测码，通过对数据计算固定长度的校验值来检测传输或存储中的错误。广泛用于网络协议和存储系统。",
      "category": "protocol",
      "source": "Peterson & Brown (1961). Cyclic Codes for Error Detection. Proceedings of the IRE",
      "notes": "CRC-32用于以太网帧校验，CRC只能检错不能纠错"
    },
    {
      "id": "汉明码",
      "name": "汉明码",
      "definition": "Hamming Code（汉明码）是一种线性纠错码，通过在数据位之间插入校验位来实现单比特纠错。是ECC内存和通信系统中最基础的纠错编码方案。",
      "category": "protocol",
      "source": "Hamming, R.W. (1950). Error Detecting and Error Correcting Codes. Bell System Technical Journal",
      "notes": "汉明距离为3的码可纠正1位错误，检测2位错误"
    },
    {
      "id": "cbfc",
      "name": "CBFC",
      "definition": "CBFC（Credit-Based Flow Control）是一种链路层流量控制机制，接收方预先通告可用缓冲区信用，发送方消耗信用后才能发送数据。确保数据不会因接收方缓冲区溢出而丢失。",
      "category": "protocol",
      "source": "InfiniBand Architecture Specification; PCIe Base Specification",
      "notes": "InfiniBand和PCIe均使用信用流控，是无损网络的基础"
    },
    {
      "id": "内存池化",
      "name": "内存池化",
      "definition": "Memory Pooling（内存池化）是将分布式节点的内存资源汇聚为统一可管理的共享内存池，由控制器动态分配给各计算节点使用。CXL 3.0的核心特性之一。",
      "category": "system",
      "source": "CXL Consortium. Compute Express Link Specification 3.0",
      "notes": "内存池化解耦计算与内存，提高资源利用率"
    },
    {
      "id": "token",
      "name": "Token",
      "definition": "Token（大模型语境）是文本经分词器处理后的最小离散符号单位，是LLM输入输出的基本对象。一个token可以是单词、子词或字符，取决于分词算法。",
      "category": "model",
      "source": "Sennrich et al. (2016). Neural Machine Translation of Rare Words with Subword Units. ACL",
      "notes": "GPT系列使用BPE分词，1 token约等于0.75个英文单词"
    },
    {
      "id": "tps",
      "name": "TPS",
      "definition": "TPS（Tokens Per Second）在LLM推理中指模型每秒生成的token数量，是衡量推理性能的核心指标。受限于内存带宽（decode阶段）和计算能力（prefill阶段）。",
      "category": "inference",
      "source": "MLPerf Inference Benchmark; LLM Inference Performance Metrics",
      "notes": "LLM推理TPS受限于HBM带宽和GPU算力"
    },
    {
      "id": "注意力机制",
      "name": "注意力机制",
      "definition": "Attention Mechanism（注意力机制）是一种让模型动态关注输入序列不同部分的技术，通过计算Query与Key的相似度来加权Value。是Transformer架构的核心组件。",
      "category": "model",
      "source": "Vaswani et al. (2017). Attention Is All You Need. NeurIPS 2017",
      "notes": "自注意力复杂度O(n²)，n为序列长度"
    },
    {
      "id": "超节点",
      "name": "超节点",
      "definition": "Super Node（超节点）是由多台计算节点通过高带宽、低延迟互连紧密耦合形成的逻辑计算单元，对上层呈现为近似单机的计算语义。典型如NVIDIA DGX SuperPOD。",
      "category": "system",
      "source": "NVIDIA DGX SuperPOD Architecture Guide",
      "notes": "超节点内部使用NVLink/NVSwitch，外部使用InfiniBand"
    },
    {
      "id": "fp4_int8",
      "name": "FP4/INT8",
      "definition": "FP4是4位浮点格式（如E2M1），INT8是8位整数格式（-128到127）。两者均用于神经网络量化，以较小精度损失换取内存和计算效率提升。",
      "category": "inference",
      "source": "NVIDIA Transformer Engine; Dettmers et al. (2022). LLM.int8(): 8-bit Matrix Multiplication. NeurIPS",
      "notes": "H100支持FP8/INT8张量核，NVIDIA Blackwell支持FP4"
    },
    {
      "id": "权重",
      "name": "权重",
      "definition": "Weight（权重）是神经网络中神经元连接的可学习参数，反映输入对输出的影响强度。LLM的权重主要分布在Attention和FFN层，存储和传输权重是主要的内存开销。",
      "category": "model",
      "source": "Goodfellow et al. (2016). Deep Learning. MIT Press",
      "notes": "70B模型FP16权重约140GB，是TP/PP切分的主要对象"
    },
    {
      "id": "激活",
      "name": "激活",
      "definition": "Activation（激活）是神经网络前向传播过程中各层的中间输出值。LLM推理中，激活内存随batch size和序列长度增长，是SP优化的主要目标。",
      "category": "model",
      "source": "Goodfellow et al. (2016). Deep Learning. MIT Press",
      "notes": "训练时需保存激活用于反向传播，推理时可流式释放"
    },
    {
      "id": "同步访问",
      "name": "同步访问",
      "definition": "Synchronous Access（同步访问）是发起访问请求后，调用方阻塞等待直到操作完成的访问模式。简单可靠但可能导致CPU空等，适用于对延迟敏感的场景。",
      "category": "system",
      "source": "Hennessy & Patterson (2017). Computer Architecture: A Quantitative Approach",
      "notes": "同步I/O会阻塞进程，影响CPU利用率"
    },
    {
      "id": "异步访问",
      "name": "异步访问",
      "definition": "Asynchronous Access（异步访问）是发起请求后立即返回，通过回调、轮询或信号通知获取结果的访问模式。允许CPU与I/O操作重叠，提高系统吞吐量。",
      "category": "system",
      "source": "Hennessy & Patterson (2017). Computer Architecture: A Quantitative Approach",
      "notes": "CUDA流、DMA传输均为异步操作的典型应用"
    },
    {
      "id": "pcs",
      "name": "PCS",
      "definition": "PCS（Physical Coding Sublayer）是以太网物理层的子层，负责数据编码/解码、加扰、块对齐等功能。是连接MAC层和物理介质的关键接口层。",
      "category": "protocol",
      "source": "IEEE 802.3 Ethernet Standard",
      "notes": "PCS实现64B/66B编码（10GbE+）或PAM4调制（100GbE+）"
    },
    {
      "id": "pfc",
      "name": "PFC",
      "definition": "PFC（Priority-based Flow Control）是IEEE 802.1Qbb定义的优先级流控机制，可对以太网的8个优先级队列独立发送PAUSE帧。是RoCE实现无损以太网的关键技术。",
      "category": "protocol",
      "source": "IEEE 802.1Qbb Priority-based Flow Control",
      "notes": "PFC可能导致队头阻塞(HoL blocking)和死锁问题"
    },
    {
      "id": "fec",
      "name": "FEC",
      "definition": "FEC（Forward Error Correction）是在发送端添加冗余编码，使接收端能检测并纠正传输错误的技术。高速链路（如100GbE+）必须使用FEC来满足误码率要求。",
      "category": "protocol",
      "source": "IEEE 802.3 Clause 91 (RS-FEC); IEEE 802.3bs",
      "notes": "RS(544,514) FEC用于100/200/400GbE，增加约2.6%开销"
    },
    {
      "id": "post方案_pcie",
      "name": "Posted方案（PCIe）",
      "definition": "Posted Transaction是PCIe中发送方不等待接收方响应即可继续的事务类型。Memory Write是Posted事务，可提高写入效率；Memory Read是Non-Posted事务，必须等待响应。",
      "category": "system",
      "source": "PCI Express Base Specification 5.0",
      "notes": "Posted事务提高吞吐但无法确认写入完成，需用Flush机制"
    },
    {
      "id": "pam4",
      "name": "PAM4",
      "definition": "PAM4（4-Level Pulse Amplitude Modulation）是使用4个电平表示2比特的调制技术。相比NRZ在相同波特率下数据速率翻倍，是100GbE+链路的主流调制方式。",
      "category": "protocol",
      "source": "IEEE 802.3bs; OIF CEI-56G-PAM4",
      "notes": "PAM4信噪比要求比NRZ高约9.5dB，更易受干扰"
    },
    {
      "id": "nrz",
      "name": "NRZ",
      "definition": "NRZ（Non-Return-to-Zero）是使用两个电平分别表示0和1的基带编码方式。每个比特周期信号电平保持恒定，是最基础的串行编码方式。",
      "category": "protocol",
      "source": "IEEE 802.3 Ethernet Standard",
      "notes": "NRZ在56Gbps以上速率难以满足误码率要求，逐渐被PAM4取代"
    },
    {
      "id": "网络隔离",
      "name": "网络隔离",
      "definition": "Network Isolation（网络隔离）是将网络划分为多个独立子网的设计策略，可通过VLAN、VRF或物理隔离实现。用于提高安全性、故障隔离和服务质量保证。",
      "category": "system",
      "source": "IEEE 802.1Q VLAN Standard; RFC 4364 (BGP/MPLS VPN)",
      "notes": "AI集群通常将管理网、存储网、计算网物理隔离"
    },
    {
      "id": "icrc",
      "name": "ICRC",
      "definition": "ICRC（Invariant CRC）是InfiniBand/RoCE协议中的端到端数据完整性校验字段，计算范围不包括会被网络设备修改的字段。确保数据在跨多跳传输后完整性。",
      "category": "protocol",
      "source": "InfiniBand Architecture Specification Volume 1",
      "notes": "ICRC补充VCRC（链路级CRC），提供端到端保护"
    },
    {
      "id": "fecn",
      "name": "FECN",
      "definition": "FECN（Forward Explicit Congestion Notification）是网络设备在数据包中标记拥塞的机制，通知接收方其路径上发生了拥塞。与ECN/BECN配合实现端到端拥塞控制。",
      "category": "protocol",
      "source": "InfiniBand Architecture Specification; RFC 3168 (ECN)",
      "notes": "InfiniBand使用FECN/BECN，以太网使用ECN"
    },
    {
      "id": "go_back_n",
      "name": "Go-Back-N",
      "definition": "Go-Back-N是一种滑动窗口ARQ协议，发送方可连续发送最多N个未确认帧。若发现错误，接收方丢弃后续所有帧，发送方需重传出错帧及其后所有帧。",
      "category": "protocol",
      "source": "Tanenbaum & Wetherall (2011). Computer Networks. Pearson",
      "notes": "实现简单但带宽利用率低于选择性重传"
    },
    {
      "id": "go_back_0",
      "name": "Go-Back-0 (Stop-and-Wait)",
      "definition": "Go-Back-0（即Stop-and-Wait）是最简单的ARQ协议，发送方每发送一帧必须等待确认才能发送下一帧。窗口大小为1，简单可靠但链路利用率低。",
      "category": "protocol",
      "source": "Tanenbaum & Wetherall (2011). Computer Networks. Pearson",
      "notes": "高延迟链路上效率极低，主要用于教学演示"
    },
    {
      "id": "psn",
      "name": "PSN",
      "definition": "PSN（Packet Sequence Number）是InfiniBand/RoCE中用于保证数据包顺序和检测丢包的序列号字段。接收方通过PSN检测乱序和丢失，触发重传。",
      "category": "protocol",
      "source": "InfiniBand Architecture Specification Volume 1",
      "notes": "PSN为24位，配合MSN确保可靠传输"
    },
    {
      "id": "urpc_华为",
      "name": "URPC（华为）",
      "definition": "URPC（Unified Remote Procedure Call）是华为开发的统一远程过程调用框架，支持同步/异步调用和多种传输协议。用于华为AI系统中模块间高效通信。",
      "category": "communication",
      "source": "Huawei MindSpore Documentation",
      "notes": "URPC针对AI场景优化，支持RDMA传输"
    },
    {
      "id": "gva",
      "name": "GVA",
      "definition": "GVA（Guest Virtual Address）是虚拟机内进程使用的虚拟地址，需经过两级地址转换（GVA→GPA→HPA）才能访问物理内存。是虚拟化环境下的用户态地址空间。",
      "category": "system",
      "source": "Intel 64 and IA-32 Architectures Software Developer's Manual Vol.3",
      "notes": "GVA到HPA需经过Guest页表和EPT/NPT两级转换"
    },
    {
      "id": "spa",
      "name": "SPA",
      "definition": "SPA（System Physical Address）是多节点系统中全局统一的物理地址空间，用于跨节点寻址。每个节点的NPA通过地址映射转换为全局SPA。",
      "category": "system",
      "source": "AMD EPYC Processor Architecture; Intel Xeon Scalable Platform",
      "notes": "SPA实现多插槽NUMA系统的统一寻址"
    },
    {
      "id": "npa",
      "name": "NPA",
      "definition": "NPA（Node Physical Address）是单个节点或芯片内部使用的本地物理地址，仅在该节点内有效。需要通过地址映射转换为SPA才能被其他节点访问。",
      "category": "system",
      "source": "AMD EPYC Processor Architecture; CXL Specification",
      "notes": "NPA到SPA的映射由系统固件或BIOS配置"
    },
    {
      "id": "mmu",
      "name": "MMU",
      "definition": "MMU（Memory Management Unit）是处理器中负责虚拟地址到物理地址转换的硬件单元，同时实现内存保护和缓存控制。通过TLB加速页表查询。",
      "category": "system",
      "source": "Hennessy & Patterson (2017). Computer Architecture: A Quantitative Approach",
      "notes": "现代MMU支持多级页表和大页(2MB/1GB)"
    },
    {
      "id": "虚拟化",
      "name": "虚拟化",
      "definition": "Virtualization（虚拟化）是通过软件抽象层将物理资源（计算、存储、网络）模拟为多个隔离的虚拟实例的技术。实现资源整合、隔离和灵活调度。",
      "category": "system",
      "source": "Popek & Goldberg (1974). Formal Requirements for Virtualizable Third Generation Architectures. CACM",
      "notes": "硬件辅助虚拟化（VT-x/AMD-V）大幅提升虚拟机性能"
    },
    {
      "id": "prefill",
      "name": "Prefill",
      "definition": "Prefill是LLM推理中对输入prompt进行一次性前向计算以构建KV Cache的阶段。计算密集型，适合GPU并行计算，延迟与prompt长度成正比。",
      "category": "inference",
      "source": "Pope et al. (2023). Efficiently Scaling Transformer Inference. MLSys",
      "notes": "Prefill阶段计算量为O(n²d)，n为prompt长度，d为隐藏维度"
    },
    {
      "id": "decode",
      "name": "Decode",
      "definition": "Decode是LLM推理中基于KV Cache逐token自回归生成输出的阶段。内存带宽瓶颈型，每生成一个token需读取全部模型权重和KV Cache。",
      "category": "inference",
      "source": "Pope et al. (2023). Efficiently Scaling Transformer Inference. MLSys",
      "notes": "Decode阶段是典型的memory-bound场景，TPS受限于HBM带宽"
    },
    {
      "id": "通信原语",
      "name": "通信原语",
      "definition": "Communication Primitive（通信原语）是分布式系统中预定义的基本数据交换操作模式，如AllReduce、AllGather、AllToAll等。是构建分布式训练和推理的基础。",
      "category": "communication",
      "source": "MPI Forum. MPI: A Message-Passing Interface Standard",
      "notes": "不同并行策略使用不同通信原语：DP用AllReduce，EP用AllToAll"
    },
    {
      "id": "flops",
      "name": "FLOPs",
      "definition": "FLOPs（Floating Point Operations）是指算法或模型执行一次所需的浮点运算次数。是衡量模型计算复杂度的标准指标，与FLOPS（每秒浮点运算数）不同。",
      "category": "hardware",
      "source": "MLPerf Benchmark Methodology",
      "notes": "Transformer前向FLOPs约为2×参数量×token数"
    },
    {
      "id": "torus",
      "name": "Torus",
      "definition": "Torus（环面）是将节点排列成多维网格并在每个维度首尾相连的网络拓扑。相比Mesh减少了直径和平均跳数，是超级计算机的常用互联拓扑。",
      "category": "interconnect",
      "source": "Dally & Towles (2004). Principles and Practices of Interconnection Networks. Morgan Kaufmann",
      "notes": "IBM Blue Gene使用3D Torus，Fugaku使用6D Torus"
    },
    {
      "id": "fat_tree",
      "name": "Fat-Tree",
      "definition": "Fat-Tree（胖树）是一种多层交换机组成的无阻塞网络拓扑，上层聚合带宽等于下层总带宽。由Leiserson在1985年提出，是数据中心网络的主流架构。",
      "category": "interconnect",
      "source": "Leiserson (1985). Fat-Trees: Universal Networks for Hardware-Efficient Supercomputing. IEEE TC",
      "notes": "Clos网络是Fat-Tree的一般化形式"
    },
    {
      "id": "dragonfly",
      "name": "Dragonfly",
      "definition": "Dragonfly（蜻蜓）是一种高效的层次化网络拓扑，将节点分组，组内全连接，组间通过全局链路稀疏连接。直径低、成本效益高，用于大规模HPC系统。",
      "category": "interconnect",
      "source": "Kim et al. (2008). Technology-Driven, Highly-Scalable Dragonfly Topology. ISCA",
      "notes": "Cray Slingshot使用Dragonfly+变体拓扑"
    },
    {
      "id": "mesh",
      "name": "Mesh",
      "definition": "Mesh（网格）是节点排列成规则多维网格、每个节点与相邻节点直接连接的拓扑。结构规则、布线简单，是片上网络（NoC）最常用的拓扑。",
      "category": "interconnect",
      "source": "Dally & Towles (2004). Principles and Practices of Interconnection Networks. Morgan Kaufmann",
      "notes": "2D Mesh直径为2(√N-1)，N为节点数"
    },
    {
      "id": "ring",
      "name": "Ring",
      "definition": "Ring（环形）是所有节点连成闭环的拓扑，数据沿环单向或双向传输。结构最简单但延迟随节点数线性增长，适用于小规模或特定通信模式。",
      "category": "interconnect",
      "source": "Dally & Towles (2004). Principles and Practices of Interconnection Networks. Morgan Kaufmann",
      "notes": "Ring AllReduce是经典的带宽最优集合通信算法"
    },
    {
      "id": "crossbar",
      "name": "Crossbar",
      "definition": "Crossbar（交叉开关）是任意输入端口可同时连接到任意输出端口的全连接交换结构。无阻塞但成本为O(N²)，适用于小规模高性能场景。",
      "category": "interconnect",
      "source": "Dally & Towles (2004). Principles and Practices of Interconnection Networks. Morgan Kaufmann",
      "notes": "NVSwitch内部使用Crossbar实现GPU全连接"
    },
    {
      "id": "nvlink",
      "name": "NVLink",
      "definition": "NVLink是NVIDIA开发的GPU高速互联技术，提供比PCIe更高的带宽（第四代单链路100GB/s）和更低延迟。用于多GPU系统内的直接通信。",
      "category": "interconnect",
      "source": "NVIDIA NVLink and NVSwitch Technology Overview",
      "notes": "H100配备18条NVLink 4.0，总带宽900GB/s双向"
    },
    {
      "id": "nvswitch",
      "name": "NVSwitch",
      "definition": "NVSwitch是NVIDIA开发的NVLink交换芯片，在多GPU系统中实现GPU间全连接通信。第三代NVSwitch支持8个GPU全速NVLink互联。",
      "category": "interconnect",
      "source": "NVIDIA NVSwitch Architecture Whitepaper",
      "notes": "DGX H100使用4个NVSwitch实现8-GPU全连接"
    },
    {
      "id": "infinity_fabric",
      "name": "Infinity Fabric",
      "definition": "Infinity Fabric是AMD开发的可扩展片上/片间互联架构，连接CPU核心、GPU CU和I/O设备。支持缓存一致性，是AMD芯粒(Chiplet)架构的基础。",
      "category": "interconnect",
      "source": "AMD Infinity Architecture Technical Overview",
      "notes": "Infinity Fabric 3.0支持CXL协议"
    },
    {
      "id": "upi",
      "name": "UPI",
      "definition": "UPI（Ultra Path Interconnect）是Intel开发的CPU间互联技术，取代QPI。用于多路服务器中CPU之间的高速通信和缓存一致性维护。",
      "category": "interconnect",
      "source": "Intel Xeon Scalable Processor Architecture",
      "notes": "第四代UPI速率16GT/s，支持最多8路互联"
    },
    {
      "id": "cxl",
      "name": "CXL",
      "definition": "CXL（Compute Express Link）是基于PCIe物理层的开放互联标准，支持CPU与加速器、内存扩展设备间的缓存一致性访问和内存共享。",
      "category": "interconnect",
      "source": "CXL Consortium. Compute Express Link Specification",
      "notes": "CXL 3.0支持内存池化和多级交换"
    },
    {
      "id": "pcie",
      "name": "PCIe",
      "definition": "PCIe（Peripheral Component Interconnect Express）是高速串行扩展总线标准，用于连接CPU与GPU、NVMe、网卡等外设。PCIe 5.0单通道速率32GT/s。",
      "category": "interconnect",
      "source": "PCI-SIG. PCI Express Base Specification",
      "notes": "x16 PCIe 5.0双向带宽128GB/s，是GPU与CPU的主要连接方式"
    },
    {
      "id": "infiniband",
      "name": "InfiniBand",
      "definition": "InfiniBand是高性能计算领域的高速互联标准，提供低延迟、高带宽的RDMA通信能力。NDR速率400Gb/s，是AI/HPC集群的主流网络。",
      "category": "interconnect",
      "source": "InfiniBand Trade Association. InfiniBand Architecture Specification",
      "notes": "InfiniBand原生支持RDMA，无需额外协议栈"
    },
    {
      "id": "roce",
      "name": "RoCE",
      "definition": "RoCE（RDMA over Converged Ethernet）是在以太网上实现RDMA的技术。RoCEv2使用UDP封装支持路由，配合PFC/ECN实现无损传输。",
      "category": "interconnect",
      "source": "InfiniBand Trade Association. RoCE Specification",
      "notes": "RoCEv2可利用现有以太网基础设施，部署成本低于InfiniBand"
    },
    {
      "id": "ethernet",
      "name": "Ethernet",
      "definition": "Ethernet（以太网）是最广泛使用的局域网技术，遵循IEEE 802.3标准。数据中心常用100/200/400/800GbE，是通用网络连接的基础。",
      "category": "interconnect",
      "source": "IEEE 802.3 Ethernet Standard",
      "notes": "以太网通过PFC/ECN实现无损特性后可支持RDMA(RoCE)"
    },
    {
      "id": "allreduce",
      "name": "AllReduce",
      "definition": "AllReduce是集合通信原语，所有进程贡献数据进行规约运算（如求和），结果分发给所有进程。数据并行训练中用于梯度同步的核心操作。",
      "category": "communication",
      "source": "MPI Forum. MPI: A Message-Passing Interface Standard",
      "notes": "Ring AllReduce通信量为2(N-1)/N × 数据量，接近带宽下界"
    },
    {
      "id": "allgather",
      "name": "AllGather",
      "definition": "AllGather是集合通信原语，每个进程发送本地数据给所有其他进程，最终每个进程拥有所有数据的完整副本。张量并行中用于收集切分的激活。",
      "category": "communication",
      "source": "MPI Forum. MPI: A Message-Passing Interface Standard",
      "notes": "AllGather通信量为(N-1) × 每进程数据量"
    },
    {
      "id": "alltoall",
      "name": "AllToAll",
      "definition": "AllToAll是集合通信原语，每个进程向每个其他进程发送不同的数据块，实现全排列数据交换。专家并行中用于token到专家的路由重分布。",
      "category": "communication",
      "source": "MPI Forum. MPI: A Message-Passing Interface Standard",
      "notes": "AllToAll对网络双分带宽要求高，Fat-Tree拓扑更适合"
    },
    {
      "id": "reduce_scatter",
      "name": "ReduceScatter",
      "definition": "ReduceScatter是集合通信原语，先对所有进程的数据进行规约，再将结果分散到各进程。是AllReduce的分解操作之一（AllReduce = ReduceScatter + AllGather）。",
      "category": "communication",
      "source": "MPI Forum. MPI: A Message-Passing Interface Standard",
      "notes": "ZeRO-2/3使用ReduceScatter分散优化器状态"
    },
    {
      "id": "broadcast",
      "name": "Broadcast",
      "definition": "Broadcast是集合通信原语，一个根进程将数据发送给所有其他进程。常用于模型参数初始化分发和配置同步。",
      "category": "communication",
      "source": "MPI Forum. MPI: A Message-Passing Interface Standard",
      "notes": "树形Broadcast可将延迟降至O(log N)"
    },
    {
      "id": "reduce",
      "name": "Reduce",
      "definition": "Reduce是集合通信原语，所有进程的数据规约到一个根进程。与AllReduce区别是结果只在根进程可用，其他进程不获得结果。",
      "category": "communication",
      "source": "MPI Forum. MPI: A Message-Passing Interface Standard",
      "notes": "Reduce+Broadcast = AllReduce"
    },
    {
      "id": "scatter",
      "name": "Scatter",
      "definition": "Scatter是集合通信原语，根进程将数据分块发送给所有进程，每个进程收到不同的数据块。是数据分发的基本操作。",
      "category": "communication",
      "source": "MPI Forum. MPI: A Message-Passing Interface Standard",
      "notes": "Scatter与Gather互为逆操作"
    },
    {
      "id": "gather",
      "name": "Gather",
      "definition": "Gather是集合通信原语，所有进程将各自的数据发送到根进程汇总。用于收集分布式计算的结果。",
      "category": "communication",
      "source": "MPI Forum. MPI: A Message-Passing Interface Standard",
      "notes": "AllGather = Gather + Broadcast"
    },
    {
      "id": "transformer",
      "name": "Transformer",
      "definition": "Transformer是一种基于自注意力机制的神经网络架构，由编码器和解码器堆叠组成。完全依赖注意力机制建模序列依赖，是现代大语言模型的基础。",
      "category": "model",
      "source": "Vaswani et al. (2017). Attention Is All You Need. NeurIPS",
      "notes": "GPT系列使用仅解码器架构，BERT使用仅编码器架构"
    },
    {
      "id": "attention",
      "name": "Attention",
      "definition": "Attention（注意力）是通过计算Query和Key的相似度来加权Value的机制，使模型能动态关注输入的相关部分。计算复杂度O(n²)，n为序列长度。",
      "category": "model",
      "source": "Vaswani et al. (2017). Attention Is All You Need. NeurIPS",
      "notes": "Scaled Dot-Product Attention: softmax(QK^T/√d_k)V"
    },
    {
      "id": "mha",
      "name": "MHA",
      "definition": "MHA（Multi-Head Attention）将注意力计算分成多个头并行执行，每个头使用独立的Q/K/V投影矩阵学习不同的注意力模式，最后拼接输出。",
      "category": "model",
      "source": "Vaswani et al. (2017). Attention Is All You Need. NeurIPS",
      "notes": "典型配置：h=32头，d_head=128，总维度4096"
    },
    {
      "id": "mqa",
      "name": "MQA",
      "definition": "MQA（Multi-Query Attention）是MHA的变体，所有Query头共享同一组Key和Value。大幅减少KV Cache内存占用，推理友好但可能损失精度。",
      "category": "model",
      "source": "Shazeer (2019). Fast Transformer Decoding: One Write-Head is All You Need. arXiv",
      "notes": "MQA的KV Cache仅为MHA的1/h（h为头数）"
    },
    {
      "id": "gqa",
      "name": "GQA",
      "definition": "GQA（Grouped Query Attention）是MHA和MQA的折中，将Query头分成若干组，每组共享一组Key/Value。平衡内存效率和模型质量。",
      "category": "model",
      "source": "Ainslie et al. (2023). GQA: Training Generalized Multi-Query Transformer Models from Multi-Head Checkpoints. EMNLP",
      "notes": "LLaMA-2 70B使用8组GQA，32个Query头共享8组KV"
    },
    {
      "id": "mla",
      "name": "MLA",
      "definition": "MLA（Multi-head Latent Attention）是DeepSeek-V2提出的注意力变体，通过低秩投影将KV压缩到更低维度的潜在空间，大幅减少KV Cache。",
      "category": "model",
      "source": "DeepSeek-AI (2024). DeepSeek-V2: A Strong, Economical, and Efficient Mixture-of-Experts Language Model",
      "notes": "MLA的KV Cache比MHA减少90%以上"
    },
    {
      "id": "moe",
      "name": "MoE",
      "definition": "MoE（Mixture of Experts）使用门控网络动态选择部分专家子网络激活，实现条件计算。可扩大模型容量而不成比例增加计算量。",
      "category": "model",
      "source": "Shazeer et al. (2017). Outrageously Large Neural Networks: The Sparsely-Gated Mixture-of-Experts Layer. ICLR",
      "notes": "Mixtral使用8个专家每次激活2个，DeepSeek-V2使用160个专家每次激活6个"
    },
    {
      "id": "ffn",
      "name": "FFN",
      "definition": "FFN（Feed-Forward Network）是Transformer中的两层全连接网络，接在Attention之后。通常隐藏维度是模型维度的4倍，占模型参数的约2/3。",
      "category": "model",
      "source": "Vaswani et al. (2017). Attention Is All You Need. NeurIPS",
      "notes": "FFN(x) = max(0, xW₁+b₁)W₂+b₂ 或使用GELU/SwiGLU激活"
    },
    {
      "id": "kv_cache",
      "name": "KV Cache",
      "definition": "KV Cache是推理时缓存已计算的Key和Value向量，避免自回归生成时重复计算历史token的注意力。是LLM推理加速的关键技术，也是主要内存瓶颈。",
      "category": "model",
      "source": "Pope et al. (2023). Efficiently Scaling Transformer Inference. MLSys",
      "notes": "KV Cache大小 = 2 × 层数 × 头数 × 序列长度 × 头维度 × 精度字节"
    },
    {
      "id": "rope",
      "name": "RoPE",
      "definition": "RoPE（Rotary Position Embedding）是通过旋转矩阵将位置信息编码到Query和Key向量的技术。支持长度外推，是LLaMA等模型的标准位置编码。",
      "category": "model",
      "source": "Su et al. (2021). RoFormer: Enhanced Transformer with Rotary Position Embedding. arXiv",
      "notes": "RoPE的相对位置信息通过Q和K的旋转角度差体现"
    },
    {
      "id": "flash_attention",
      "name": "FlashAttention",
      "definition": "FlashAttention是一种IO感知的精确注意力算法，通过分块计算、内核融合和重计算策略减少HBM访问次数。显著加速训练和推理，节省内存。",
      "category": "inference",
      "source": "Dao et al. (2022). FlashAttention: Fast and Memory-Efficient Exact Attention with IO-Awareness. NeurIPS",
      "notes": "FlashAttention-2进一步优化，达到理论峰值性能的50-73%"
    },
    {
      "id": "paged_attention",
      "name": "PagedAttention",
      "definition": "PagedAttention是vLLM提出的KV Cache管理技术，将KV Cache分成固定大小的页，实现动态分配和跨请求共享。解决内存碎片问题，提高吞吐量。",
      "category": "inference",
      "source": "Kwon et al. (2023). Efficient Memory Management for Large Language Model Serving with PagedAttention. SOSP",
      "notes": "PagedAttention使KV Cache利用率从20-40%提升至近100%"
    },
    {
      "id": "speculative_decoding",
      "name": "Speculative Decoding",
      "definition": "Speculative Decoding（推测解码）用小模型快速生成多个候选token，再由大模型并行验证。通过一次前向验证多个token，加速自回归生成。",
      "category": "inference",
      "source": "Leviathan et al. (2023). Fast Inference from Transformers via Speculative Decoding. ICML",
      "notes": "加速比取决于小模型与大模型的token接受率"
    },
    {
      "id": "continuous_batching",
      "name": "Continuous Batching",
      "definition": "Continuous Batching（连续批处理）是动态调整推理批次的技术，已完成的请求立即退出，新请求随时加入。最大化GPU利用率，提高服务吞吐量。",
      "category": "inference",
      "source": "Yu et al. (2022). Orca: A Distributed Serving System for Transformer-Based Generative Models. OSDI",
      "notes": "也称Iteration-level Batching或In-flight Batching"
    },
    {
      "id": "quantization",
      "name": "Quantization",
      "definition": "Quantization（量化）将模型权重和激活从高精度（FP32/FP16）转为低精度（INT8/INT4/FP8），减少内存占用和计算量，可能轻微损失精度。",
      "category": "inference",
      "source": "Dettmers et al. (2022). LLM.int8(): 8-bit Matrix Multiplication for Transformers at Scale. NeurIPS",
      "notes": "PTQ(训练后量化)简单但精度损失大，QAT(量化感知训练)效果更好"
    },
    {
      "id": "gradient_checkpointing",
      "name": "Gradient Checkpointing",
      "definition": "Gradient Checkpointing（梯度检查点）只保存部分层的激活值，反向传播时重新计算未保存的激活。用内存换计算，使有限GPU内存可训练更大模型。",
      "category": "inference",
      "source": "Chen et al. (2016). Training Deep Nets with Sublinear Memory Cost. arXiv",
      "notes": "激活内存从O(n)降至O(√n)，计算量增加约33%"
    },
    {
      "id": "zero",
      "name": "ZeRO",
      "definition": "ZeRO（Zero Redundancy Optimizer）是DeepSpeed提出的优化器状态分片技术，将优化器状态、梯度和参数分片到多个GPU，消除数据并行中的冗余存储。",
      "category": "inference",
      "source": "Rajbhandari et al. (2020). ZeRO: Memory Optimizations Toward Training Trillion Parameter Models. SC",
      "notes": "ZeRO-1/2/3分别分片优化器状态/梯度/参数"
    },
    {
      "id": "hbm",
      "name": "HBM",
      "definition": "HBM（High Bandwidth Memory）是采用3D堆叠技术的高带宽内存，通过硅中介层与处理器紧密连接。是GPU/NPU的主要显存技术，提供TB/s级带宽。",
      "category": "hardware",
      "source": "JEDEC. High Bandwidth Memory (HBM) DRAM Standard",
      "notes": "H100使用HBM3(80GB, 3.35TB/s)，H200使用HBM3e(141GB, 4.8TB/s)"
    },
    {
      "id": "npu",
      "name": "NPU",
      "definition": "NPU（Neural Processing Unit）是专门为神经网络计算优化的AI加速芯片，通常包含大规模矩阵乘法单元和片上内存。典型如华为Ascend、寒武纪MLU。",
      "category": "hardware",
      "source": "IEEE Computer Society. AI Accelerator Architecture Survey",
      "notes": "NPU针对推理优化，强调能效比；GPU更通用"
    },
    {
      "id": "gpu",
      "name": "GPU",
      "definition": "GPU（Graphics Processing Unit）是大规模并行处理器，包含数千个计算核心，是深度学习训练和推理的主力硬件。NVIDIA在AI GPU市场占主导地位。",
      "category": "hardware",
      "source": "NVIDIA. GPU Architecture Whitepapers",
      "notes": "H100有132个SM，16896个CUDA核心，FP16性能1979 TFLOPS"
    },
    {
      "id": "tpu",
      "name": "TPU",
      "definition": "TPU（Tensor Processing Unit）是Google为机器学习设计的ASIC加速器，采用脉动阵列架构优化矩阵运算。TPU v5e的BF16性能197 TFLOPS。",
      "category": "hardware",
      "source": "Jouppi et al. (2017). In-Datacenter Performance Analysis of a Tensor Processing Unit. ISCA",
      "notes": "TPU通过ICI互联组成Pod，最大支持数千TPU"
    },
    {
      "id": "systolic_array",
      "name": "Systolic Array",
      "definition": "Systolic Array（脉动阵列）是一种高效的矩阵乘法硬件架构，数据在处理单元阵列中有规律地流动，每个PE执行乘加操作并传递结果。是TPU核心计算单元。",
      "category": "hardware",
      "source": "Kung (1982). Why Systolic Architectures? IEEE Computer",
      "notes": "脉动阵列数据复用率高，适合矩阵密集计算"
    },
    {
      "id": "roofline",
      "name": "Roofline Model",
      "definition": "Roofline Model（屋顶线模型）是用于分析程序性能瓶颈的可视化模型，横轴为算术强度(FLOPs/Byte)，纵轴为性能(FLOPS)。性能受限于计算峰值和内存带宽中较低者。",
      "category": "hardware",
      "source": "Williams et al. (2009). Roofline: An Insightful Visual Performance Model for Multicore Architectures. CACM",
      "notes": "Decode阶段算术强度低，是memory-bound；Prefill阶段算术强度高，接近compute-bound"
    },
    {
      "id": "nccl",
      "name": "NCCL",
      "definition": "NCCL（NVIDIA Collective Communications Library）是NVIDIA开发的多GPU集合通信库，针对NVLink和InfiniBand优化。是PyTorch/TensorFlow分布式训练的默认后端。",
      "category": "communication",
      "source": "NVIDIA. NCCL Documentation",
      "notes": "NCCL支持AllReduce、AllGather、ReduceScatter等全部集合操作"
    },
    {
      "id": "gloo",
      "name": "Gloo",
      "definition": "Gloo是Facebook开发的集合通信库，支持CPU和GPU，提供TCP和共享内存传输。是PyTorch分布式训练的CPU后端，也支持GPU（但性能低于NCCL）。",
      "category": "communication",
      "source": "Facebook. Gloo GitHub Repository",
      "notes": "Gloo可在无InfiniBand环境下使用TCP进行分布式训练"
    },
    {
      "id": "mpi",
      "name": "MPI",
      "definition": "MPI（Message Passing Interface）是分布式计算的标准通信接口规范，定义了点对点和集合通信原语。OpenMPI和MPICH是主要实现。",
      "category": "communication",
      "source": "MPI Forum. MPI: A Message-Passing Interface Standard",
      "notes": "MPI定义了200+接口，是HPC领域的事实标准"
    },
    {
      "id": "rdma",
      "name": "RDMA",
      "definition": "RDMA（Remote Direct Memory Access）是允许一台计算机直接访问另一台计算机内存的技术，绕过操作系统内核和远程CPU，实现微秒级延迟通信。",
      "category": "communication",
      "source": "InfiniBand Trade Association. InfiniBand Architecture Specification",
      "notes": "RDMA有三种实现：InfiniBand、RoCE、iWARP"
    },
    {
      "id": "gpudirect",
      "name": "GPUDirect",
      "definition": "GPUDirect是NVIDIA的GPU直接通信技术族，包括P2P（GPU间直接传输）、RDMA（GPU直接访问网络）和Storage（GPU直接访问存储）。消除CPU中转开销。",
      "category": "communication",
      "source": "NVIDIA. GPUDirect Technology Overview",
      "notes": "GPUDirect RDMA允许网卡直接读写GPU内存"
    }
  ],
  "relations": [
    {"source": "tp", "target": "sp", "type": "complements", "label": "配合使用"},
    {"source": "tp", "target": "dp", "type": "combines_with", "label": "组合使用"},
    {"source": "tp", "target": "pp", "type": "combines_with", "label": "组合使用"},
    {"source": "ep", "target": "moe", "type": "enables", "label": "支持"},
    {"source": "dp", "target": "allreduce", "type": "uses", "label": "梯度同步"},
    {"source": "tp", "target": "allreduce", "type": "uses", "label": "激活同步"},
    {"source": "tp", "target": "allgather", "type": "uses", "label": "激活收集"},
    {"source": "ep", "target": "alltoall", "type": "uses", "label": "专家路由"},
    {"source": "scale_up", "target": "nvlink", "type": "implemented_by", "label": "典型实现"},
    {"source": "scale_out", "target": "infiniband", "type": "implemented_by", "label": "典型实现"},
    {"source": "scale_out", "target": "ethernet", "type": "implemented_by", "label": "典型实现"},
    {"source": "nvlink", "target": "nvswitch", "type": "connects_via", "label": "交换芯片"},
    {"source": "nvlink", "target": "gpu", "type": "connects", "label": "连接"},
    {"source": "pcie", "target": "cxl", "type": "basis_for", "label": "物理层基础"},
    {"source": "infiniband", "target": "rdma", "type": "supports", "label": "原生支持"},
    {"source": "roce", "target": "rdma", "type": "implements", "label": "以太网实现"},
    {"source": "roce", "target": "ethernet", "type": "runs_on", "label": "运行于"},
    {"source": "ethernet", "target": "pfc", "type": "uses", "label": "无损流控"},
    {"source": "torus", "target": "mesh", "type": "extends", "label": "首尾相连"},
    {"source": "fat_tree", "target": "pod", "type": "used_in", "label": "数据中心"},
    {"source": "ring", "target": "allreduce", "type": "optimizes", "label": "Ring AllReduce"},
    {"source": "dragonfly", "target": "fat_tree", "type": "alternative_to", "label": "替代方案"},
    {"source": "allreduce", "target": "reduce_scatter", "type": "decomposes_to", "label": "分解"},
    {"source": "allreduce", "target": "allgather", "type": "decomposes_to", "label": "分解"},
    {"source": "reduce", "target": "allreduce", "type": "part_of", "label": "组成"},
    {"source": "broadcast", "target": "allreduce", "type": "part_of", "label": "组成"},
    {"source": "gather", "target": "scatter", "type": "inverse_of", "label": "逆操作"},
    {"source": "transformer", "target": "attention", "type": "contains", "label": "核心组件"},
    {"source": "transformer", "target": "ffn", "type": "contains", "label": "核心组件"},
    {"source": "mha", "target": "attention", "type": "implements", "label": "多头实现"},
    {"source": "mqa", "target": "mha", "type": "variant_of", "label": "变体"},
    {"source": "gqa", "target": "mha", "type": "variant_of", "label": "变体"},
    {"source": "mla", "target": "mha", "type": "variant_of", "label": "变体"},
    {"source": "moe", "target": "ffn", "type": "replaces", "label": "替代FFN"},
    {"source": "kv_cache", "target": "attention", "type": "caches", "label": "缓存KV"},
    {"source": "kv_cache", "target": "decode", "type": "accelerates", "label": "加速"},
    {"source": "rope", "target": "attention", "type": "enhances", "label": "位置编码"},
    {"source": "flash_attention", "target": "attention", "type": "optimizes", "label": "IO优化"},
    {"source": "flash_attention", "target": "hbm", "type": "reduces_access", "label": "减少访问"},
    {"source": "paged_attention", "target": "kv_cache", "type": "manages", "label": "内存管理"},
    {"source": "speculative_decoding", "target": "decode", "type": "accelerates", "label": "加速"},
    {"source": "continuous_batching", "target": "decode", "type": "optimizes", "label": "批处理优化"},
    {"source": "quantization", "target": "权重", "type": "compresses", "label": "压缩"},
    {"source": "quantization", "target": "激活", "type": "compresses", "label": "压缩"},
    {"source": "zero", "target": "dp", "type": "optimizes", "label": "内存优化"},
    {"source": "gradient_checkpointing", "target": "激活", "type": "trades_off", "label": "内存换计算"},
    {"source": "hbm", "target": "gpu", "type": "memory_of", "label": "显存"},
    {"source": "hbm", "target": "npu", "type": "memory_of", "label": "显存"},
    {"source": "tpu", "target": "systolic_array", "type": "uses", "label": "核心架构"},
    {"source": "roofline", "target": "hbm", "type": "analyzes", "label": "带宽分析"},
    {"source": "roofline", "target": "flops", "type": "analyzes", "label": "算力分析"},
    {"source": "nccl", "target": "allreduce", "type": "implements", "label": "实现"},
    {"source": "nccl", "target": "nvlink", "type": "optimized_for", "label": "优化"},
    {"source": "nccl", "target": "gpu", "type": "runs_on", "label": "运行于"},
    {"source": "gloo", "target": "mpi", "type": "similar_to", "label": "类似接口"},
    {"source": "rdma", "target": "gpudirect", "type": "enables", "label": "支持"},
    {"source": "gpudirect", "target": "gpu", "type": "accelerates", "label": "加速通信"},
    {"source": "ecc", "target": "汉明码", "type": "based_on", "label": "基础算法"},
    {"source": "fec", "target": "ecc", "type": "type_of", "label": "前向纠错"},
    {"source": "pfc", "target": "cbfc", "type": "similar_to", "label": "流控机制"},
    {"source": "fecn", "target": "pfc", "type": "works_with", "label": "配合"},
    {"source": "prefill", "target": "decode", "type": "precedes", "label": "阶段顺序"},
    {"source": "prefill", "target": "kv_cache", "type": "builds", "label": "构建缓存"},
    {"source": "decode", "target": "token", "type": "generates", "label": "生成"},
    {"source": "decode", "target": "tps", "type": "measured_by", "label": "性能指标"},
    {"source": "gva", "target": "mmu", "type": "translated_by", "label": "地址转换"},
    {"source": "spa", "target": "npa", "type": "maps_to", "label": "地址映射"},
    {"source": "mmu", "target": "虚拟化", "type": "enables", "label": "支持"},
    {"source": "pod", "target": "rack", "type": "contains", "label": "包含"},
    {"source": "rack", "target": "server", "type": "contains", "label": "包含"},
    {"source": "server", "target": "gpu", "type": "contains", "label": "包含"},
    {"source": "1u_2u_4u", "target": "rack", "type": "unit_of", "label": "高度单位"},
    {"source": "cbfc", "target": "rdma", "type": "used_by", "label": "流控"},
    {"source": "cbfc", "target": "pcie", "type": "used_by", "label": "流控"},
    {"source": "icrc", "target": "infiniband", "type": "used_by", "label": "校验"},
    {"source": "icrc", "target": "roce", "type": "used_by", "label": "校验"},
    {"source": "psn", "target": "infiniband", "type": "used_by", "label": "序列号"},
    {"source": "内存语义", "target": "rdma", "type": "feature_of", "label": "核心特性"},
    {"source": "消息语义", "target": "mpi", "type": "feature_of", "label": "核心特性"},
    {"source": "ras", "target": "ecc", "type": "includes", "label": "包含技术"},
    {"source": "ras", "target": "server", "type": "requirement_of", "label": "设计要求"},
    {"source": "内存池化", "target": "cxl", "type": "enabled_by", "label": "CXL支持"},
    {"source": "超节点", "target": "pod", "type": "type_of", "label": "类型"},
    {"source": "超节点", "target": "nvlink", "type": "uses", "label": "内部互联"},
    {"source": "同步访问", "target": "异步访问", "type": "contrast_with", "label": "对比"},
    {"source": "pcs", "target": "pam4", "type": "implements", "label": "编码实现"},
    {"source": "pcs", "target": "nrz", "type": "implements", "label": "编码实现"},
    {"source": "pcs", "target": "ethernet", "type": "part_of", "label": "物理层"},
    {"source": "pam4", "target": "nrz", "type": "succeeds", "label": "取代"},
    {"source": "网络隔离", "target": "pod", "type": "used_in", "label": "应用场景"},
    {"source": "go_back_n", "target": "go_back_0", "type": "extends", "label": "扩展"},
    {"source": "post方案_pcie", "target": "pcie", "type": "feature_of", "label": "事务类型"},
    {"source": "post方案_pcie", "target": "事务", "type": "type_of", "label": "事务类型"},
    {"source": "通信原语", "target": "allreduce", "type": "includes", "label": "包含"},
    {"source": "通信原语", "target": "allgather", "type": "includes", "label": "包含"},
    {"source": "通信原语", "target": "alltoall", "type": "includes", "label": "包含"},
    {"source": "通信原语", "target": "broadcast", "type": "includes", "label": "包含"},
    {"source": "mpi", "target": "通信原语", "type": "defines", "label": "定义标准"},
    {"source": "transformer", "target": "注意力机制", "type": "based_on", "label": "基于"},
    {"source": "attention", "target": "注意力机制", "type": "implements", "label": "实现"},
    {"source": "mqa", "target": "kv_cache", "type": "reduces", "label": "减少内存"},
    {"source": "gqa", "target": "kv_cache", "type": "reduces", "label": "减少内存"},
    {"source": "mla", "target": "kv_cache", "type": "reduces", "label": "大幅减少"},
    {"source": "kv_cache", "target": "token", "type": "stores_for", "label": "存储"},
    {"source": "fp4_int8", "target": "quantization", "type": "format_for", "label": "量化格式"},
    {"source": "urpc_华为", "target": "rdma", "type": "uses", "label": "传输层"},
    {"source": "crossbar", "target": "nvswitch", "type": "used_in", "label": "内部结构"},
    {"source": "upi", "target": "server", "type": "connects_in", "label": "多路互联"},
    {"source": "infinity_fabric", "target": "gpu", "type": "connects", "label": "AMD互联"},
    {"source": "crc", "target": "icrc", "type": "basis_for", "label": "基础"},
    {"source": "ethernet", "target": "crc", "type": "uses", "label": "帧校验"},
    {"source": "crc", "target": "ecc", "type": "complements", "label": "检错vs纠错"},
    {"source": "权重", "target": "tp", "type": "partitioned_by", "label": "切分对象"},
    {"source": "激活", "target": "sp", "type": "optimized_by", "label": "优化对象"},
    {"source": "虚拟化", "target": "server", "type": "feature_of", "label": "服务器特性"},
    {"source": "go_back_n", "target": "psn", "type": "uses", "label": "序列号重传"},
    {"source": "异步访问", "target": "rdma", "type": "enabled_by", "label": "异步通信"},
    {"source": "通信原语", "target": "gather", "type": "includes", "label": "包含"},
    {"source": "通信原语", "target": "scatter", "type": "includes", "label": "包含"},
    {"source": "ring", "target": "mesh", "type": "simplifies_to", "label": "简化形式"},
    {"source": "tpu", "target": "hbm", "type": "uses", "label": "高带宽内存"},
    {"source": "spa", "target": "cxl", "type": "used_in", "label": "内存扩展寻址"}
  ],
  "metadata": {
    "version": "3.0.0",
    "nodeCount": 102,
    "relationCount": 122,
    "lastUpdated": "2025-01-06",
    "categories": ["hardware", "interconnect", "parallel", "communication", "model", "inference", "protocol", "system"],
    "description": "LLM系统架构知识图谱 - 8个核心分组：硬件、互联、并行、通信、模型、推理、协议、系统"
  }
}
